{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **FLUX UPSCALER - 2x EVALUATION MODE (Local - RTX 5060 Ti)**\n\n## Purpose\nThis notebook upscales images to **2x the target print size** for quality evaluation in Photoshop.\n\n- 11x14\" target â†’ outputs at 22x28\" for inspection\n- 10x20\" target â†’ outputs at 20x40\" for inspection\n\nThis allows you to zoom to 100% in Photoshop and check for pixelation before committing to the final upscale.\n\n## Setup Instructions\n1. Run the **Setup Environment** cell first (only needed once)\n2. Set your `IMAGE_CONFIGS` with the **original target sizes** (not 2x)\n3. The notebook automatically calculates 2x dimensions\n4. Run the **Upscale** cell\n\n## Notes\n- Models are downloaded to `./hf/` directory (relative to this repo)\n- Output is saved with `_2x-eval` suffix for easy identification\n- Use the standard notebook for final production output"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\Armaan\\Desktop\\Artinafti\n",
      "Models directory: C:\\Users\\Armaan\\Desktop\\Artinafti\\hf\\models\n",
      "Output directory: C:\\Users\\Armaan\\Desktop\\Artinafti\\4xoutput\n",
      "Installing pip packages...\n",
      "Installing PyTorch nightly with CUDA 12.8 (for RTX 50-series)...\n",
      "âœ“ PyTorch nightly with CUDA 12.8 installed\n",
      "âœ“ torchsde installed\n",
      "âœ“ av installed\n",
      "âœ“ diffusers installed\n",
      "âœ“ accelerate installed\n",
      "âœ“ einops installed\n",
      "âœ“ spandrel installed\n",
      "âœ“ opencv-python installed\n",
      "âœ“ imageio installed\n",
      "âœ“ imageio-ffmpeg installed\n",
      "âœ“ huggingface_hub installed\n",
      "âœ“ safetensors installed\n",
      "âœ“ gguf installed\n",
      "âœ“ sentencepiece installed\n",
      "\n",
      "âœ… Environment Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Environment (Run Once)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get repo root (parent of resolution-upscaling folder)\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "# Directory paths\n",
    "HF_DIR = REPO_ROOT / \"hf\"\n",
    "COMFYUI_DIR = HF_DIR / \"ComfyUI\"\n",
    "MODELS_DIR = HF_DIR / \"models\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"4xoutput\"\n",
    "INPUT_DIR = REPO_ROOT / \"input\"\n",
    "\n",
    "# Create directories\n",
    "for d in [HF_DIR, MODELS_DIR, OUTPUT_DIR, INPUT_DIR, \n",
    "          MODELS_DIR / \"upscale_models\", MODELS_DIR / \"unet\", \n",
    "          MODELS_DIR / \"vae\", MODELS_DIR / \"clip\", MODELS_DIR / \"loras\"]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def install_pip_packages():\n",
    "    # Install PyTorch NIGHTLY with CUDA 12.8 (required for RTX 5060 Ti sm_120 Blackwell)\n",
    "    print(\"Installing PyTorch nightly with CUDA 12.8 (for RTX 50-series)...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q', '--pre',\n",
    "         'torch', 'torchvision', 'torchaudio',\n",
    "         '--index-url', 'https://download.pytorch.org/whl/nightly/cu128'],\n",
    "        capture_output=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ“ PyTorch nightly with CUDA 12.8 installed\")\n",
    "    else:\n",
    "        print(f\"âœ— PyTorch install failed: {result.stderr.decode()}\")\n",
    "        raise RuntimeError(\"PyTorch CUDA installation failed\")\n",
    "    \n",
    "    # Other packages (xformers removed - optional and causes conflicts)\n",
    "    packages = [\n",
    "        'torchsde',\n",
    "        'av',\n",
    "        'diffusers',\n",
    "        'accelerate',\n",
    "        'einops',\n",
    "        'spandrel',\n",
    "        'opencv-python',\n",
    "        'imageio',\n",
    "        'imageio-ffmpeg',\n",
    "        'huggingface_hub',\n",
    "        'safetensors',\n",
    "        'gguf',\n",
    "        'sentencepiece',\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                check=True,\n",
    "                capture_output=True\n",
    "            )\n",
    "            print(f\"âœ“ {package} installed\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âœ— Error installing {package}: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
    "\n",
    "print(\"Installing pip packages...\")\n",
    "install_pip_packages()\n",
    "\n",
    "# Clone ComfyUI and custom nodes if not present\n",
    "if not COMFYUI_DIR.exists():\n",
    "    print(\"Cloning ComfyUI...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/Isi-dev/ComfyUI', str(COMFYUI_DIR)], check=True)\n",
    "    \n",
    "custom_nodes_dir = COMFYUI_DIR / \"custom_nodes\"\n",
    "custom_nodes_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if not (custom_nodes_dir / \"ComfyUI_GGUF\").exists():\n",
    "    print(\"Cloning ComfyUI_GGUF...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/Isi-dev/ComfyUI_GGUF.git'], cwd=str(custom_nodes_dir), check=True)\n",
    "    # Install GGUF requirements\n",
    "    req_file = custom_nodes_dir / \"ComfyUI_GGUF\" / \"requirements.txt\"\n",
    "    if req_file.exists():\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', str(req_file)], check=True)\n",
    "\n",
    "if not (custom_nodes_dir / \"ComfyUI_UltimateSDUpscale\").exists():\n",
    "    print(\"Cloning ComfyUI_UltimateSDUpscale...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/Isi-dev/ComfyUI_UltimateSDUpscale'], cwd=str(custom_nodes_dir), check=True)\n",
    "\n",
    "print(\"\\nâœ… Environment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading upscale models...\n",
      "âœ“ 4x-UltraSharp.pth already exists\n",
      "âœ“ 4x_foolhardy_Remacri.pth already exists\n",
      "âœ“ 4x-AnimeSharp.pth already exists\n",
      "\n",
      "Downloading FLUX model...\n",
      "âœ“ flux1-dev-Q8_0.gguf already exists\n",
      "\n",
      "Downloading VAE...\n",
      "âœ“ ae.sft already exists\n",
      "\n",
      "Downloading CLIP models...\n",
      "âœ“ clip_l.safetensors already exists\n",
      "âœ“ t5xxl_fp8_e4m3fn.safetensors already exists\n",
      "\n",
      "âœ… All models downloaded!\n"
     ]
    }
   ],
   "source": [
    "# @title Download Models (Run Once)\n",
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Get paths from previous cell\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "HF_DIR = REPO_ROOT / \"hf\"\n",
    "MODELS_DIR = HF_DIR / \"models\"\n",
    "\n",
    "# Create model subdirectories\n",
    "UPSCALE_MODELS_DIR = MODELS_DIR / \"upscale_models\"\n",
    "UNET_DIR = MODELS_DIR / \"unet\"\n",
    "VAE_DIR = MODELS_DIR / \"vae\"\n",
    "CLIP_DIR = MODELS_DIR / \"clip\"\n",
    "LORAS_DIR = MODELS_DIR / \"loras\"\n",
    "\n",
    "for d in [UPSCALE_MODELS_DIR, UNET_DIR, VAE_DIR, CLIP_DIR, LORAS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "download_face_upscalers = False  # Set to True if you want face upscalers\n",
    "download_loRA = False  # Set to True if you want LoRA\n",
    "\n",
    "def download_model(repo_id: str, filename: str, dest_dir: Path, subfolder: str = None) -> str:\n",
    "    \"\"\"Download model from HuggingFace Hub.\"\"\"\n",
    "    dest_path = dest_dir / filename\n",
    "    if dest_path.exists():\n",
    "        print(f\"âœ“ {filename} already exists\")\n",
    "        return filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
    "        hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=filename if subfolder is None else f\"{subfolder}/{filename}\",\n",
    "            local_dir=str(dest_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        # Move file if it was downloaded to subfolder\n",
    "        if subfolder:\n",
    "            src = dest_dir / subfolder / filename\n",
    "            if src.exists():\n",
    "                src.rename(dest_path)\n",
    "                (dest_dir / subfolder).rmdir()\n",
    "        print(\"Done!\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Downloading upscale models...\")\n",
    "x_UltraSharp = download_model(\"Isi99999/Upscalers\", \"4x-UltraSharp.pth\", UPSCALE_MODELS_DIR)\n",
    "x_foolhardy_Remacri = download_model(\"Isi99999/Upscalers\", \"4x_foolhardy_Remacri.pth\", UPSCALE_MODELS_DIR)\n",
    "x_AnimeSharp = download_model(\"Isi99999/Upscalers\", \"4x-AnimeSharp.pth\", UPSCALE_MODELS_DIR)\n",
    "\n",
    "if download_face_upscalers:\n",
    "    x_FaceUpSharpDAT = download_model(\"Isi99999/Upscalers\", \"4xFaceUpSharpDAT.pth\", UPSCALE_MODELS_DIR)\n",
    "    x_FaceUpSharpLDAT = download_model(\"Isi99999/Upscalers\", \"4xFaceUpSharpLDAT.safetensors\", UPSCALE_MODELS_DIR)\n",
    "\n",
    "print(\"\\nDownloading FLUX model...\")\n",
    "flux_model = download_model(\"city96/FLUX.1-dev-gguf\", \"flux1-dev-Q8_0.gguf\", UNET_DIR)\n",
    "\n",
    "print(\"\\nDownloading VAE...\")\n",
    "flux_vae = download_model(\"Isi99999/Upscalers\", \"ae.sft\", VAE_DIR, subfolder=\"Flux\")\n",
    "\n",
    "print(\"\\nDownloading CLIP models...\")\n",
    "flux_clip_l = download_model(\"Isi99999/Upscalers\", \"clip_l.safetensors\", CLIP_DIR, subfolder=\"Flux\")\n",
    "flux_t5xxl = download_model(\"Isi99999/Upscalers\", \"t5xxl_fp8_e4m3fn.safetensors\", CLIP_DIR, subfolder=\"Flux\")\n",
    "\n",
    "if download_loRA:\n",
    "    print(\"\\nDownloading LoRA...\")\n",
    "    flux_lora = download_model(\"Isi99999/Upscalers\", \"flux_realism_lora.safetensors\", LORAS_DIR, subfolder=\"Flux\")\n",
    "\n",
    "print(\"\\nâœ… All models downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Load Libraries and Initialize\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport gc\nimport os\nimport sys\nimport random\nimport imageio\nimport cv2\nimport shutil\nimport datetime\nfrom pathlib import Path\nfrom IPython.display import display, HTML, Image as IPImage\n\n# Set up paths\nNOTEBOOK_DIR = Path(os.getcwd()).resolve()\nif NOTEBOOK_DIR.name == \"resolution-upscaling\":\n    REPO_ROOT = NOTEBOOK_DIR.parent\nelse:\n    REPO_ROOT = NOTEBOOK_DIR\n\nHF_DIR = REPO_ROOT / \"hf\"\nCOMFYUI_DIR = HF_DIR / \"ComfyUI\"\nMODELS_DIR = HF_DIR / \"models\"\nOUTPUT_DIR = REPO_ROOT / \"4xoutput\"\nINPUT_DIR = REPO_ROOT / \"input\"\n\n# Model paths\nUPSCALE_MODELS_DIR = MODELS_DIR / \"upscale_models\"\nUNET_DIR = MODELS_DIR / \"unet\"\nVAE_DIR = MODELS_DIR / \"vae\"\nCLIP_DIR = MODELS_DIR / \"clip\"\nLORAS_DIR = MODELS_DIR / \"loras\"\n\n# Add ComfyUI to path\nsys.path.insert(0, str(COMFYUI_DIR))\n\n# Configure ComfyUI folder_paths to use our custom model directories\nimport folder_paths\nfolder_paths.folder_names_and_paths[\"text_encoders\"] = ([str(CLIP_DIR)], folder_paths.supported_pt_extensions)\nfolder_paths.folder_names_and_paths[\"clip\"] = ([str(CLIP_DIR)], folder_paths.supported_pt_extensions)\nfolder_paths.folder_names_and_paths[\"vae\"] = ([str(VAE_DIR)], folder_paths.supported_pt_extensions)\nfolder_paths.folder_names_and_paths[\"diffusion_models\"] = ([str(UNET_DIR)], folder_paths.supported_pt_extensions)\nfolder_paths.folder_names_and_paths[\"unet\"] = ([str(UNET_DIR)], folder_paths.supported_pt_extensions)\nfolder_paths.folder_names_and_paths[\"upscale_models\"] = ([str(UPSCALE_MODELS_DIR)], folder_paths.supported_pt_extensions)\nfolder_paths.folder_names_and_paths[\"loras\"] = ([str(LORAS_DIR)], folder_paths.supported_pt_extensions)\n\n# Import ComfyUI modules\nfrom nodes import (\n    DualCLIPLoader,\n    UNETLoader,\n    VAELoader,\n    LoraLoaderModelOnly,\n    LoadImage,\n    SaveImage\n)\n\nfrom custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\nfrom comfy_extras.nodes_upscale_model import UpscaleModelLoader\nfrom comfy_extras.nodes_flux import CLIPTextEncodeFlux\nfrom custom_nodes.ComfyUI_UltimateSDUpscale.nodes import (\n    UltimateSDUpscale,\n    UltimateSDUpscaleNoUpscale\n)\n\n# Model filenames\nflux_model = \"flux1-dev-Q8_0.gguf\"\nflux_vae = \"ae.sft\"\nflux_clip_l = \"clip_l.safetensors\"\nflux_t5xxl = \"t5xxl_fp8_e4m3fn.safetensors\"\nlora = None  # Set if using LoRA\n\n# Initialize loaders\nclip_loader = DualCLIPLoader()\nunet_loader = UnetLoaderGGUF()\nvae_loader = VAELoader()\nload_lora = LoraLoaderModelOnly()\nload_image = LoadImage()\nsave_image = SaveImage()\nupscale_model_loader = UpscaleModelLoader()\npositive_prompt_encode = CLIPTextEncodeFlux()\nnegative_prompt_encode = CLIPTextEncodeFlux()\nupscaler = UltimateSDUpscale()\nnoUpscale = UltimateSDUpscaleNoUpscale()\n\n# Helper functions\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n    gc.collect()\n\ndef save_as_image(image, filename_prefix, output_dir=None, formats=None):\n    \"\"\"Save single frame as PNG and/or TIFF image.\n\n    Args:\n        image: Image tensor to save\n        filename_prefix: Base filename without extension\n        output_dir: Output directory (defaults to OUTPUT_DIR)\n        formats: List of formats to save, e.g. [\"png\", \"tiff\"] (defaults to [\"png\", \"tiff\"])\n\n    Returns:\n        List of saved file paths\n    \"\"\"\n    if formats is None:\n        formats = [\"png\", \"tiff\"]\n    if output_dir is None:\n        output_dir = OUTPUT_DIR\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n    pil_img = Image.fromarray(frame)\n\n    saved_paths = []\n    for fmt in formats:\n        fmt_lower = fmt.lower()\n        if fmt_lower == \"png\":\n            output_path = output_dir / f\"{filename_prefix}.png\"\n            pil_img.save(str(output_path), \"PNG\")\n            saved_paths.append(str(output_path))\n            print(f\"   Saved: {output_path.name}\")\n        elif fmt_lower in [\"tiff\", \"tif\"]:\n            output_path = output_dir / f\"{filename_prefix}.tiff\"\n            pil_img.save(str(output_path), \"TIFF\", compression=None)\n            saved_paths.append(str(output_path))\n            print(f\"   Saved: {output_path.name}\")\n\n    return saved_paths\n\ndef save_as_mp4(images, filename_prefix, fps, output_dir=None):\n    \"\"\"Save frames as MP4 video.\"\"\"\n    if output_dir is None:\n        output_dir = OUTPUT_DIR\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    output_path = output_dir / f\"{filename_prefix}.mp4\"\n\n    frames = []\n    for i, img in enumerate(images):\n        try:\n            if isinstance(img, torch.Tensor):\n                img = img.cpu().numpy()\n\n            if img.max() <= 1.0:\n                img = (img * 255).astype(np.uint8)\n            else:\n                img = img.astype(np.uint8)\n\n            if len(img.shape) == 4:\n                img = img[0]\n\n            if len(img.shape) == 3:\n                if img.shape[0] in (1, 3, 4):\n                    img = np.transpose(img, (1, 2, 0))\n                elif img.shape[2] > 4:\n                    img = img[:, :, :3]\n            elif len(img.shape) == 2:\n                img = np.expand_dims(img, axis=-1)\n\n            if len(img.shape) != 3 or img.shape[2] not in (1, 3, 4):\n                raise ValueError(f\"Invalid frame shape after processing: {img.shape}\")\n\n            frames.append(img)\n        except Exception as e:\n            print(f\"Error processing frame {i}: {str(e)}\")\n            raise\n\n    with imageio.get_writer(str(output_path), fps=fps) as writer:\n        for frame in frames:\n            writer.append_data(frame)\n\n    return str(output_path)\n\ndef extract_frames(video_path, max_frames=None):\n    \"\"\"Extract frames from video and return as a list of images.\"\"\"\n    vidcap = cv2.VideoCapture(str(video_path))\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    frames = []\n\n    while True:\n        success, frame = vidcap.read()\n        if not success or (max_frames and len(frames) >= max_frames):\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frames.append(frame)\n\n    vidcap.release()\n    if not frames:\n        return None, fps\n\n    print(f\"Extracted {len(frames)} frames\")\n    return frames, fps\n\ndef select_every_n_frame(frames, fps, n, skip_first=0, max_output_frames=0):\n    \"\"\"Select every nth frame from video.\"\"\"\n    if not frames or n < 1:\n        raise ValueError(\"Frames must be a non-empty list and n must be >= 1\")\n\n    frames_to_use = frames[skip_first:]\n    if not frames_to_use:\n        print(\"No frames available after skipping.\")\n        return [], 0.0\n\n    selected_frames = frames_to_use[::n]\n    if max_output_frames != 0 and len(selected_frames) > max_output_frames:\n        selected_frames = selected_frames[:max_output_frames]\n\n    adjusted_fps = fps / n\n    print(f\"Adjusted FPS: {adjusted_fps:.2f} -> Final output: {len(selected_frames)} frames\")\n\n    return selected_frames, adjusted_fps\n\ndef display_video(video_path):\n    \"\"\"Display video in notebook.\"\"\"\n    from base64 import b64encode\n\n    video_data = open(video_path, 'rb').read()\n    if video_path.lower().endswith('.mp4'):\n        mime_type = \"video/mp4\"\n    elif video_path.lower().endswith('.webm'):\n        mime_type = \"video/webm\"\n    else:\n        mime_type = \"video/mp4\"\n\n    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n    display(HTML(f\"\"\"\n    <video width=512 controls autoplay loop>\n        <source src=\"{data_url}\" type=\"{mime_type}\">\n    </video>\n    \"\"\"))\n\ndef remove_frame(image_path: str, threshold: float = 0.15, min_frame_width: int = 10, max_frame_width: int = 200) -> str:\n    \"\"\"\n    Remove frame/border from an image by detecting and cropping out the frame.\n    \n    Uses edge detection and variance analysis to find where the frame ends and content begins.\n    \n    Args:\n        image_path: Path to input image\n        threshold: Edge detection threshold (0.0-1.0), lower = more sensitive\n        min_frame_width: Minimum expected frame width in pixels\n        max_frame_width: Maximum expected frame width in pixels\n    \n    Returns:\n        Path to the cropped image (saves to same location with '_no_frame' suffix)\n    \"\"\"\n    image_path = Path(image_path)\n    if not image_path.exists():\n        raise FileNotFoundError(f\"Image not found: {image_path}\")\n    \n    # Load image\n    img = cv2.imread(str(image_path))\n    if img is None:\n        raise ValueError(f\"Could not load image: {image_path}\")\n    \n    original_height, original_width = img.shape[:2]\n    \n    # Convert to grayscale for analysis\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    def find_frame_edge(scan_range: int, is_horizontal: bool, from_start: bool) -> int:\n        \"\"\"\n        Find frame edge by scanning from edge inward.\n        \n        Args:\n            scan_range: How many pixels to scan from the edge\n            is_horizontal: True for top/bottom edges, False for left/right\n            from_start: True for top/left, False for bottom/right\n        \n        Returns:\n            Pixel position where content starts (frame ends)\n        \"\"\"\n        strip_width = 10\n        max_variance_pos = 0\n        max_variance = 0\n        \n        if is_horizontal:\n            if from_start:\n                # Scan from top\n                for y in range(0, min(scan_range, original_height - strip_width), 2):\n                    strip = gray[y:y+strip_width, :]\n                    variance = np.var(strip)\n                    if variance > max_variance:\n                        max_variance = variance\n                        max_variance_pos = y\n            else:\n                # Scan from bottom\n                for y in range(original_height - min(scan_range, original_height), original_height - strip_width, -2):\n                    strip = gray[y:y+strip_width, :]\n                    variance = np.var(strip)\n                    if variance > max_variance:\n                        max_variance = variance\n                        max_variance_pos = y\n        else:\n            if from_start:\n                # Scan from left\n                for x in range(0, min(scan_range, original_width - strip_width), 2):\n                    strip = gray[:, x:x+strip_width]\n                    variance = np.var(strip)\n                    if variance > max_variance:\n                        max_variance = variance\n                        max_variance_pos = x\n            else:\n                # Scan from right\n                for x in range(original_width - min(scan_range, original_width), original_width - strip_width, -2):\n                    strip = gray[:, x:x+strip_width]\n                    variance = np.var(strip)\n                    if variance > max_variance:\n                        max_variance = variance\n                        max_variance_pos = x\n        \n        return max_variance_pos\n    \n    # Find frame edges\n    scan_range = min(max_frame_width, original_width // 3, original_height // 3)\n    \n    top = find_frame_edge(scan_range, True, True)\n    bottom = find_frame_edge(scan_range, True, False)\n    left = find_frame_edge(scan_range, False, True)\n    right = find_frame_edge(scan_range, False, False)\n    \n    # Ensure minimum frame width\n    if top < min_frame_width:\n        top = min_frame_width\n    if left < min_frame_width:\n        left = min_frame_width\n    if original_height - bottom < min_frame_width:\n        bottom = original_height - min_frame_width\n    if original_width - right < min_frame_width:\n        right = original_width - min_frame_width\n    \n    # Ensure valid bounds\n    top = max(0, min(top, original_height - 20))\n    bottom = max(top + 20, min(bottom, original_height))\n    left = max(0, min(left, original_width - 20))\n    right = max(left + 20, min(right, original_width))\n    \n    # Validate crop size (must be at least 50% of original)\n    content_width = right - left\n    content_height = bottom - top\n    \n    if content_width < original_width * 0.5 or content_height < original_height * 0.5:\n        # Fallback: conservative crop (remove 3% from each side)\n        margin = min(original_width, original_height) // 30\n        top = margin\n        bottom = original_height - margin\n        left = margin\n        right = original_width - margin\n        print(\"Warning: Frame detection may have failed, using conservative crop\")\n    \n    # Crop the image\n    cropped = img[top:bottom, left:right]\n    \n    # Save cropped image\n    output_path = image_path.parent / f\"{image_path.stem}_no_frame{image_path.suffix}\"\n    cv2.imwrite(str(output_path), cropped)\n    \n    print(f\"Removed frame: cropped from {original_width}x{original_height} to {content_width}x{content_height}\")\n    print(f\"Removed margins: top={top}, bottom={original_height-bottom}, left={left}, right={original_width-right}\")\n    \n    return str(output_path)\n\nprint(f\"âœ… Libraries loaded!\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Configuration - 2x EVALUATION MODE\n\n# ============== 2x EVALUATION MODE ==============\n# This notebook upscales to 2x the target print size for quality inspection.\n# Enter your ORIGINAL target sizes below - the notebook automatically doubles them.\n#\n# Examples:\n#   11x14\" target â†’ evaluates at 22x28\"\n#   10x20\" target â†’ evaluates at 20x40\"\n\nEVALUATION_MULTIPLIER = 2  # 2x target dimensions for quality testing\n\nDPI = 150\n\nIMAGE_CONFIGS = [\n    {\n        \"image_id\": \"b46bc519-9b4b-487f-b8d4-b95195d7e02e\",\n        \"input_path\": \"input/b46bc519-9b4b-487f-b8d4-b95195d7e02e.jpg\",\n        \"width_inches\": 20,   # ORIGINAL target (will evaluate at 40\")\n        \"height_inches\": 10,  # ORIGINAL target (will evaluate at 20\")\n    },\n]\n\n# Calculate 2x evaluation dimensions\nfor config in IMAGE_CONFIGS:\n    config[\"original_width\"] = config[\"width_inches\"]\n    config[\"original_height\"] = config[\"height_inches\"]\n    config[\"eval_width_inches\"] = config[\"width_inches\"] * EVALUATION_MULTIPLIER\n    config[\"eval_height_inches\"] = config[\"height_inches\"] * EVALUATION_MULTIPLIER\n    config[\"target_width_px\"] = config[\"eval_width_inches\"] * DPI\n    config[\"target_height_px\"] = config[\"eval_height_inches\"] * DPI\n    config[\"target_aspect\"] = config[\"width_inches\"] / config[\"height_inches\"]\n\n\ndef calculate_scale_for_crop(\n    input_width: int,\n    input_height: int,\n    target_width_inches: float,\n    target_height_inches: float,\n    dpi: int\n) -> dict:\n    \"\"\"\n    Calculate the scale factor and output dimensions needed to ensure\n    we have enough resolution after cropping to the target aspect ratio.\n    \"\"\"\n    # Target final dimensions\n    final_width_px = int(target_width_inches * dpi)\n    final_height_px = int(target_height_inches * dpi)\n    \n    # Aspect ratios (width/height)\n    input_aspect = input_width / input_height\n    target_aspect = target_width_inches / target_height_inches\n    \n    if abs(input_aspect - target_aspect) < 0.01:\n        scale_factor = final_width_px / input_width\n        return {\n            \"output_width_px\": final_width_px,\n            \"output_height_px\": final_height_px,\n            \"final_width_px\": final_width_px,\n            \"final_height_px\": final_height_px,\n            \"scale_factor\": scale_factor,\n            \"crop_direction\": \"none\",\n            \"crop_amount_px\": 0,\n            \"crop_amount_inches\": 0.0,\n            \"equivalent_print_size\": f\"{target_width_inches}\\\" x {target_height_inches}\\\"\"\n        }\n    \n    if input_aspect < target_aspect:\n        output_width_px = final_width_px\n        scale_factor = output_width_px / input_width\n        output_height_px = int(input_height * scale_factor)\n        \n        crop_amount_px = output_height_px - final_height_px\n        crop_amount_inches = crop_amount_px / dpi\n        crop_direction = \"vertical\"\n        equivalent_height_inches = output_height_px / dpi\n        equivalent_print_size = f\"{target_width_inches}\\\" x {equivalent_height_inches:.2f}\\\"\"\n    else:\n        output_height_px = final_height_px\n        scale_factor = output_height_px / input_height\n        output_width_px = int(input_width * scale_factor)\n        \n        crop_amount_px = output_width_px - final_width_px\n        crop_amount_inches = crop_amount_px / dpi\n        crop_direction = \"horizontal\"\n        equivalent_width_inches = output_width_px / dpi\n        equivalent_print_size = f\"{equivalent_width_inches:.2f}\\\" x {target_height_inches}\\\"\"\n    \n    return {\n        \"output_width_px\": output_width_px,\n        \"output_height_px\": output_height_px,\n        \"final_width_px\": final_width_px,\n        \"final_height_px\": final_height_px,\n        \"scale_factor\": scale_factor,\n        \"crop_direction\": crop_direction,\n        \"crop_amount_px\": crop_amount_px,\n        \"crop_amount_inches\": crop_amount_inches,\n        \"equivalent_print_size\": equivalent_print_size\n    }\n\n\n# ============== OUTPUT FORMAT SETTINGS ==============\nOUTPUT_FORMATS = [\"png\", \"tiff\"]\n\n# ============== PROMPT SETTINGS ==============\npositive_prompt = \"\"\npositive_prompt2 = \"\"\nnegative_prompt = \"\"\nnegative_prompt2 = \"\"\nguidance = 3.5\n\n# ============== UPSCALE SETTINGS ==============\nupscale_by = 4\nseed = 0\nsteps = 20\ncfg = 7\nsampler_name = \"euler\"\nscheduler = \"normal\"\ndenoise = 0.2\n\nupscale_model = \"4x-UltraSharp.pth\"\n\n# ============== TILE SETTINGS ==============\nmode_type = \"Linear\"\ntile_width = 512\ntile_height = 512\nmask_blur = 8\ntile_padding = 32\n\n# ============== SEAM FIX SETTINGS ==============\nseam_fix_mode = \"None\"\nseam_fix_denoise = 1.0\nseam_fix_width = 64\nseam_fix_mask_blur = 8\nseam_fix_padding = 16\nforce_uniform_tiles = True\ntiled_decode = False\n\n# ============== LoRA SETTINGS ==============\nuse_loRA = False\nLoRA_Strength = 1.0\n\n# ============== FRAME REMOVAL SETTINGS ==============\nremove_frame_before_upscale = False\nframe_removal_threshold = 0.15\nmin_frame_width = 10\nmax_frame_width = 200\n\nprint(\"=\" * 60)\nprint(\"2x EVALUATION MODE - Configuration\")\nprint(\"=\" * 60)\nprint(f\"Model: {upscale_model}\")\nprint(f\"Upscale by: {upscale_by}x\")\nprint(f\"DPI: {DPI}\")\nprint(f\"Evaluation multiplier: {EVALUATION_MULTIPLIER}x\")\nprint(f\"Output formats: {OUTPUT_FORMATS}\")\nprint(f\"\\nImages to process (2x evaluation sizes):\")\nfor config in IMAGE_CONFIGS:\n    print(f\"  {config['image_id'][:8]}...\")\n    print(f\"    Original target: {config['original_width']}\\\" x {config['original_height']}\\\"\")\n    print(f\"    Evaluation size: {config['eval_width_inches']}\\\" x {config['eval_height_inches']}\\\" @ {DPI}DPI\")\n    print(f\"    Output pixels: {config['target_width_px']}x{config['target_height_px']}px\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Run 2x Evaluation Upscaling with FLUX\n\ndef resize_to_dimensions(image_tensor: torch.Tensor, target_width: int, target_height: int) -> torch.Tensor:\n    \"\"\"Resize image tensor to exact target dimensions using high-quality Lanczos.\"\"\"\n    img_np = (image_tensor[0].cpu().numpy() * 255).astype(np.uint8)\n    pil_img = Image.fromarray(img_np)\n    pil_img = pil_img.resize((target_width, target_height), Image.LANCZOS)\n    resized_np = np.array(pil_img).astype(np.float32) / 255.0\n    resized_tensor = torch.from_numpy(resized_np).unsqueeze(0)\n    return resized_tensor\n\n\ndef upscale_single_image(\n    image_path: str,\n    target_width_inches: float,\n    target_height_inches: float,\n    original_width_inches: float,\n    original_height_inches: float,\n    dpi: int,\n    output_name: str,\n    output_formats: list,\n    positive_prompt: str,\n    positive_prompt2: str,\n    negative_prompt: str,\n    negative_prompt2: str,\n    guidance: float,\n    upscale_by: float,\n    seed: int,\n    steps: int,\n    cfg: float,\n    sampler_name: str,\n    scheduler: str,\n    denoise: float,\n    upscale_model_name: str,\n    mode_type: str,\n    tile_width: int,\n    tile_height: int,\n    mask_blur: int,\n    tile_padding: int,\n    seam_fix_mode: str,\n    seam_fix_denoise: float,\n    seam_fix_width: int,\n    seam_fix_mask_blur: int,\n    seam_fix_padding: int,\n    force_uniform_tiles: bool,\n    tiled_decode: bool,\n    use_lora: bool,\n    lora_strength: float,\n    clip, model, vae, upscale_model_load, positive, negative\n) -> dict:\n    \"\"\"Upscale a single image at 2x evaluation size.\"\"\"\n    import time\n    start_time = time.time()\n    \n    image_path = Path(image_path)\n    if not image_path.is_absolute():\n        image_path = REPO_ROOT / image_path\n    \n    if not image_path.exists():\n        raise FileNotFoundError(f\"Input file not found: {image_path}\")\n    \n    print(f\"Processing: {image_path.name}\")\n    print(f\"Original target: {original_width_inches}\\\" x {original_height_inches}\\\"\")\n    print(f\"Evaluation size: {target_width_inches}\\\" x {target_height_inches}\\\" @ {dpi} DPI\")\n    \n    loaded_image = load_image.load_image(str(image_path))[0]\n    \n    input_height, input_width = loaded_image.shape[1], loaded_image.shape[2]\n    input_aspect = input_width / input_height\n    target_aspect = target_width_inches / target_height_inches\n    print(f\"Input size: {input_width}x{input_height} (aspect: {input_aspect:.3f})\")\n    \n    scale_info = calculate_scale_for_crop(\n        input_width=input_width,\n        input_height=input_height,\n        target_width_inches=target_width_inches,\n        target_height_inches=target_height_inches,\n        dpi=dpi\n    )\n    \n    output_width = scale_info[\"output_width_px\"]\n    output_height = scale_info[\"output_height_px\"]\n    \n    print(f\"\\nðŸ“ 2x Evaluation Scaling:\")\n    print(f\"   Output: {output_width}x{output_height}px ({scale_info['equivalent_print_size']})\")\n    \n    upscaled_w = int(input_width * upscale_by)\n    upscaled_h = int(input_height * upscale_by)\n    print(f\"\\n   Step 1: FLUX upscale {upscale_by}x -> {upscaled_w}x{upscaled_h}\")\n    print(f\"   Step 2: Resize to evaluation size -> {output_width}x{output_height}\")\n    \n    print(\"\\nUpscaling with FLUX...\")\n    upscale_start = time.time()\n    \n    image_out = upscaler.upscale(\n        image=loaded_image,\n        model=model,\n        positive=positive,\n        negative=negative,\n        vae=vae,\n        upscale_by=upscale_by,\n        seed=seed,\n        steps=steps,\n        cfg=cfg,\n        sampler_name=sampler_name,\n        scheduler=scheduler,\n        denoise=denoise,\n        upscale_model=upscale_model_load,\n        mode_type=mode_type,\n        tile_width=tile_width,\n        tile_height=tile_height,\n        mask_blur=mask_blur,\n        tile_padding=tile_padding,\n        seam_fix_mode=seam_fix_mode,\n        seam_fix_denoise=seam_fix_denoise,\n        seam_fix_mask_blur=seam_fix_mask_blur,\n        seam_fix_width=seam_fix_width,\n        seam_fix_padding=seam_fix_padding,\n        force_uniform_tiles=force_uniform_tiles,\n        tiled_decode=tiled_decode,\n    )[0]\n    \n    print(f\"FLUX upscaling took: {time.time() - upscale_start:.1f}s\")\n    \n    print(f\"Resizing to evaluation size: {output_width}x{output_height}...\")\n    image_resized = resize_to_dimensions(image_out, output_width, output_height)\n    \n    print(f\"\\nSaving outputs ({', '.join(output_formats)})...\")\n    output_paths = save_as_image(image_resized[0], output_name, formats=output_formats)\n    \n    print(f\"\\nâœ… Done!\")\n    print(f\"Output size: {output_width}x{output_height}px\")\n    print(f\"Evaluation print size: {scale_info['equivalent_print_size']} @ {dpi} DPI\")\n    print(f\"Total time: {time.time() - start_time:.1f}s\")\n    \n    return {\n        \"output_paths\": output_paths,\n        \"output_width\": output_width,\n        \"output_height\": output_height,\n        \"equivalent_print_size\": scale_info[\"equivalent_print_size\"]\n    }\n\n\n# ============== BATCH PROCESSING ==============\nprint(\"=\" * 60)\nprint(\"2x EVALUATION MODE - FLUX UPSCALING\")\nprint(\"=\" * 60)\n\nwith torch.inference_mode():\n    print(\"\\nLoading models...\")\n    \n    print(\"Loading Text_Encoder...\")\n    clip = clip_loader.load_clip(flux_t5xxl, flux_clip_l, \"flux\")[0]\n    \n    positive = positive_prompt_encode.encode(clip, positive_prompt, positive_prompt2, guidance)[0]\n    negative = negative_prompt_encode.encode(clip, negative_prompt, negative_prompt2, guidance)[0]\n    \n    del clip\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(\"Loading Unet Model...\")\n    model = unet_loader.load_unet(flux_model)[0]\n    \n    print(\"Loading upscale model...\")\n    upscale_model_load = upscale_model_loader.load_model(upscale_model)[0]\n    \n    print(\"Loading VAE...\")\n    vae = vae_loader.load_vae(flux_vae)[0]\n    \n    if use_loRA and lora is not None:\n        print(\"Loading LoRA...\")\n        model = load_lora.load_lora_model_only(model, lora, LoRA_Strength)[0]\n    \n    print(\"\\nâœ… Models loaded!\\n\")\n    \n    results = []\n    \n    for i, config in enumerate(IMAGE_CONFIGS, 1):\n        print(f\"\\n{'=' * 60}\")\n        print(f\"[{i}/{len(IMAGE_CONFIGS)}] {config['image_id']}\")\n        print(f\"Original: {config['original_width']}\\\" x {config['original_height']}\\\"\")\n        print(f\"Evaluating at: {config['eval_width_inches']}\\\" x {config['eval_height_inches']}\\\"\")\n        print(f\"{'=' * 60}\")\n        \n        actual_seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n        print(f\"Using seed: {actual_seed}\")\n        \n        try:\n            # Use _2x-eval suffix for evaluation outputs\n            output_name = f\"{config['image_id']}_flux-upscaler_2x-eval\"\n            \n            result = upscale_single_image(\n                image_path=config[\"input_path\"],\n                target_width_inches=config[\"eval_width_inches\"],\n                target_height_inches=config[\"eval_height_inches\"],\n                original_width_inches=config[\"original_width\"],\n                original_height_inches=config[\"original_height\"],\n                dpi=DPI,\n                output_name=output_name,\n                output_formats=OUTPUT_FORMATS,\n                positive_prompt=positive_prompt,\n                positive_prompt2=positive_prompt2,\n                negative_prompt=negative_prompt,\n                negative_prompt2=negative_prompt2,\n                guidance=guidance,\n                upscale_by=upscale_by,\n                seed=actual_seed,\n                steps=steps,\n                cfg=cfg,\n                sampler_name=sampler_name,\n                scheduler=scheduler,\n                denoise=denoise,\n                upscale_model_name=upscale_model,\n                mode_type=mode_type,\n                tile_width=tile_width,\n                tile_height=tile_height,\n                mask_blur=mask_blur,\n                tile_padding=tile_padding,\n                seam_fix_mode=seam_fix_mode,\n                seam_fix_denoise=seam_fix_denoise,\n                seam_fix_width=seam_fix_width,\n                seam_fix_mask_blur=seam_fix_mask_blur,\n                seam_fix_padding=seam_fix_padding,\n                force_uniform_tiles=force_uniform_tiles,\n                tiled_decode=tiled_decode,\n                use_lora=use_loRA,\n                lora_strength=LoRA_Strength,\n                clip=None,\n                model=model,\n                vae=vae,\n                upscale_model_load=upscale_model_load,\n                positive=positive,\n                negative=negative\n            )\n            \n            results.append({\n                \"image_id\": config[\"image_id\"],\n                \"status\": \"Success\",\n                \"outputs\": result[\"output_paths\"],\n                \"output_size\": f\"{result['output_width']}x{result['output_height']}\",\n                \"eval_size\": result[\"equivalent_print_size\"],\n                \"original_target\": f\"{config['original_width']}\\\" x {config['original_height']}\\\"\"\n            })\n            \n        except Exception as e:\n            print(f\"\\nâŒ Error: {e}\")\n            import traceback\n            traceback.print_exc()\n            results.append({\n                \"image_id\": config[\"image_id\"],\n                \"status\": \"Failed\",\n                \"error\": str(e)\n            })\n    \n    del model, vae, upscale_model_load, positive, negative\n    clear_memory()\n\n# Summary\nprint(f\"\\n\\n{'=' * 60}\")\nprint(\"2x EVALUATION - COMPLETE\")\nprint(f\"{'=' * 60}\")\n\nsuccess_count = sum(1 for r in results if r[\"status\"] == \"Success\")\nprint(f\"\\nProcessed: {success_count}/{len(results)} images successfully\\n\")\n\nfor r in results:\n    if r[\"status\"] == \"Success\":\n        print(f\"âœ… {r['image_id'][:8]}...\")\n        for out_path in r[\"outputs\"]:\n            print(f\"   Output: {Path(out_path).name}\")\n        print(f\"   Size: {r['output_size']} ({r['eval_size']} @ {DPI} DPI)\")\n        print(f\"   Original target: {r['original_target']}\")\n        print()\n    else:\n        print(f\"âŒ {r['image_id'][:8]}... -> {r.get('error', 'Unknown error')}\")\n\nprint(\"\\nðŸ“‹ Next steps:\")\nprint(\"   1. Open the output in Photoshop\")\nprint(\"   2. View at 100% to check for pixelation\")\nprint(\"   3. If quality is acceptable, run the standard notebook for final output\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}