{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Real-ESRGAN Upscaler - 2x EVALUATION MODE (Fast)**\n",
    "\n",
    "## Purpose\n",
    "This notebook upscales images to **2x the target print size** for quality evaluation in Photoshop.\n",
    "\n",
    "- 11x14\" target â†’ outputs at 22x28\" for inspection\n",
    "- 10x20\" target â†’ outputs at 20x40\" for inspection\n",
    "\n",
    "## Speed Comparison\n",
    "- **This notebook**: ~30-120 seconds per image (fast evaluation)\n",
    "- **FLUX notebook**: ~20-40 minutes per image (higher quality)\n",
    "\n",
    "## When to Use\n",
    "- Quick quality checks before final FLUX upscale\n",
    "- Preview pixelation at full print size\n",
    "- Fast iteration on multiple images\n",
    "\n",
    "## Setup Instructions\n",
    "1. Run the **Setup Environment** cell first (only needed once)\n",
    "2. Set your `IMAGE_CONFIGS` with the **original target sizes** (not 2x)\n",
    "3. The notebook automatically calculates 2x dimensions\n",
    "4. Run the **Upscale** cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\Armaan\\Desktop\\Artinafti\n",
      "Models directory: C:\\Users\\Armaan\\Desktop\\Artinafti\\hf\\models\\upscale_models\n",
      "Output directory: C:\\Users\\Armaan\\Desktop\\Artinafti\\4xoutput\n",
      "Installing packages...\n",
      "Installing PyTorch nightly with CUDA 12.8...\n",
      "âœ“ PyTorch installed\n",
      "âœ“ spandrel installed\n",
      "âœ“ opencv-python installed\n",
      "âœ“ huggingface_hub installed\n",
      "âœ“ safetensors installed\n",
      "âœ“ pillow installed\n",
      "\n",
      "âœ… Environment Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Environment (Run Once)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get repo root\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "# Directory paths\n",
    "HF_DIR = REPO_ROOT / \"hf\"\n",
    "MODELS_DIR = HF_DIR / \"models\"\n",
    "UPSCALE_MODELS_DIR = MODELS_DIR / \"upscale_models\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"4xoutput\"\n",
    "INPUT_DIR = REPO_ROOT / \"input\"\n",
    "\n",
    "# Create directories\n",
    "for d in [HF_DIR, MODELS_DIR, UPSCALE_MODELS_DIR, OUTPUT_DIR, INPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Models directory: {UPSCALE_MODELS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for Real-ESRGAN standalone.\"\"\"\n",
    "    # PyTorch with CUDA 12.8 for RTX 5060 Ti\n",
    "    print(\"Installing PyTorch nightly with CUDA 12.8...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q', '--pre',\n",
    "         'torch', 'torchvision',\n",
    "         '--index-url', 'https://download.pytorch.org/whl/nightly/cu128'],\n",
    "        capture_output=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ“ PyTorch installed\")\n",
    "    else:\n",
    "        print(f\"âœ— PyTorch install failed: {result.stderr.decode()}\")\n",
    "    \n",
    "    # Spandrel for loading upscale models\n",
    "    packages = ['spandrel', 'opencv-python', 'huggingface_hub', 'safetensors', 'pillow']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                check=True, capture_output=True\n",
    "            )\n",
    "            print(f\"âœ“ {package} installed\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âœ— Error installing {package}\")\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "install_packages()\n",
    "print(\"\\nâœ… Environment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading upscale models...\n",
      "âœ“ 4x-UltraSharp.pth already exists\n",
      "âœ“ 4x_foolhardy_Remacri.pth already exists\n",
      "âœ“ 4x-AnimeSharp.pth already exists\n",
      "\n",
      "âœ… Models downloaded!\n"
     ]
    }
   ],
   "source": [
    "# @title Download Models (Run Once)\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Ensure paths are set\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "UPSCALE_MODELS_DIR = REPO_ROOT / \"hf\" / \"models\" / \"upscale_models\"\n",
    "UPSCALE_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_model(repo_id: str, filename: str, dest_dir: Path) -> str:\n",
    "    \"\"\"Download model from HuggingFace Hub.\"\"\"\n",
    "    dest_path = dest_dir / filename\n",
    "    if dest_path.exists():\n",
    "        print(f\"âœ“ {filename} already exists\")\n",
    "        return filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
    "        hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=filename,\n",
    "            local_dir=str(dest_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        print(\"Done!\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Downloading upscale models...\")\n",
    "download_model(\"Isi99999/Upscalers\", \"4x-UltraSharp.pth\", UPSCALE_MODELS_DIR)\n",
    "download_model(\"Isi99999/Upscalers\", \"4x_foolhardy_Remacri.pth\", UPSCALE_MODELS_DIR)\n",
    "download_model(\"Isi99999/Upscalers\", \"4x-AnimeSharp.pth\", UPSCALE_MODELS_DIR)\n",
    "\n",
    "print(\"\\nâœ… Models downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded!\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "VRAM: 15.9 GB\n"
     ]
    }
   ],
   "source": [
    "# @title Load Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Import spandrel for model loading\n",
    "from spandrel import ImageModelDescriptor, ModelLoader\n",
    "\n",
    "# Set up paths\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "UPSCALE_MODELS_DIR = REPO_ROOT / \"hf\" / \"models\" / \"upscale_models\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"4xoutput\"\n",
    "INPUT_DIR = REPO_ROOT / \"input\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"âœ… Libraries loaded!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2x EVALUATION MODE - Configuration\n",
      "============================================================\n",
      "Model: 4x-UltraSharp.pth\n",
      "Two-pass (16x): True\n",
      "DPI: 150\n",
      "Evaluation multiplier: 2x\n",
      "Output formats: ['png', 'tiff']\n",
      "\n",
      "Images to process (2x evaluation sizes):\n",
      "  7a49a140...\n",
      "    Original target: 20\" x 10\"\n",
      "    Evaluation size: 40\" x 20\" @ 150DPI\n",
      "    Output pixels: 6000x3000px\n",
      "  df5cc4ff...\n",
      "    Original target: 20\" x 10\"\n",
      "    Evaluation size: 40\" x 20\" @ 150DPI\n",
      "    Output pixels: 6000x3000px\n",
      "  b46bc519...\n",
      "    Original target: 20\" x 10\"\n",
      "    Evaluation size: 40\" x 20\" @ 150DPI\n",
      "    Output pixels: 6000x3000px\n"
     ]
    }
   ],
   "source": [
    "# @title Configuration - 2x EVALUATION MODE\n",
    "\n",
    "# ============== 2x EVALUATION MODE ==============\n",
    "# This notebook upscales to 2x the target print size for quality inspection.\n",
    "# Enter your ORIGINAL target sizes below - the notebook automatically doubles them.\n",
    "#\n",
    "# Examples:\n",
    "#   11x14\" target â†’ evaluates at 22x28\"\n",
    "#   10x20\" target â†’ evaluates at 20x40\"\n",
    "\n",
    "EVALUATION_MULTIPLIER = 2  # 2x target dimensions for quality testing\n",
    "\n",
    "DPI = 150\n",
    "\n",
    "IMAGE_CONFIGS = [\n",
    "    {\n",
    "        \"image_id\": \"7a49a140-beee-4141-9593-3fddb9194240\",\n",
    "        \"input_path\": \"input/7a49a140-beee-4141-9593-3fddb9194240.jpg\",\n",
    "        \"width_inches\": 20,   # ORIGINAL target (will evaluate at 40\")\n",
    "        \"height_inches\": 10,  # ORIGINAL target (will evaluate at 20\")\n",
    "    },\n",
    "    {\n",
    "        \"image_id\": \"df5cc4ff-9846-412c-ad72-e53c42184b9c\",\n",
    "        \"input_path\": \"input/df5cc4ff-9846-412c-ad72-e53c42184b9c.jpg\",\n",
    "        \"width_inches\": 20,   # ORIGINAL target (will evaluate at 40\")\n",
    "        \"height_inches\": 10,  # ORIGINAL target (will evaluate at 20\")\n",
    "    },\n",
    "    {\n",
    "        \"image_id\": \"b46bc519-9b4b-487f-b8d4-b95195d7e02e\",\n",
    "        \"input_path\": \"input/b46bc519-9b4b-487f-b8d4-b95195d7e02e.jpg\",\n",
    "        \"width_inches\": 20,   # ORIGINAL target (will evaluate at 40\")\n",
    "        \"height_inches\": 10,  # ORIGINAL target (will evaluate at 20\")\n",
    "    },\n",
    "]\n",
    "\n",
    "# Use 16x upscaling (two passes of 4x model)\n",
    "USE_TWO_PASS = True\n",
    "\n",
    "# Calculate 2x evaluation dimensions\n",
    "for config in IMAGE_CONFIGS:\n",
    "    config[\"original_width\"] = config[\"width_inches\"]\n",
    "    config[\"original_height\"] = config[\"height_inches\"]\n",
    "    config[\"eval_width_inches\"] = config[\"width_inches\"] * EVALUATION_MULTIPLIER\n",
    "    config[\"eval_height_inches\"] = config[\"height_inches\"] * EVALUATION_MULTIPLIER\n",
    "    config[\"target_width_px\"] = config[\"eval_width_inches\"] * DPI\n",
    "    config[\"target_height_px\"] = config[\"eval_height_inches\"] * DPI\n",
    "    config[\"target_aspect\"] = config[\"width_inches\"] / config[\"height_inches\"]\n",
    "\n",
    "\n",
    "def calculate_scale_for_crop(\n",
    "    input_width: int,\n",
    "    input_height: int,\n",
    "    target_width_inches: float,\n",
    "    target_height_inches: float,\n",
    "    dpi: int\n",
    ") -> dict:\n",
    "    \"\"\"Calculate aspect-ratio-aware output dimensions.\"\"\"\n",
    "    final_width_px = int(target_width_inches * dpi)\n",
    "    final_height_px = int(target_height_inches * dpi)\n",
    "    \n",
    "    input_aspect = input_width / input_height\n",
    "    target_aspect = target_width_inches / target_height_inches\n",
    "    \n",
    "    if abs(input_aspect - target_aspect) < 0.01:\n",
    "        scale_factor = final_width_px / input_width\n",
    "        return {\n",
    "            \"output_width_px\": final_width_px,\n",
    "            \"output_height_px\": final_height_px,\n",
    "            \"final_width_px\": final_width_px,\n",
    "            \"final_height_px\": final_height_px,\n",
    "            \"scale_factor\": scale_factor,\n",
    "            \"crop_direction\": \"none\",\n",
    "            \"crop_amount_px\": 0,\n",
    "            \"crop_amount_inches\": 0.0,\n",
    "            \"equivalent_print_size\": f\"{target_width_inches}\\\" x {target_height_inches}\\\"\"\n",
    "        }\n",
    "    \n",
    "    if input_aspect < target_aspect:\n",
    "        output_width_px = final_width_px\n",
    "        scale_factor = output_width_px / input_width\n",
    "        output_height_px = int(input_height * scale_factor)\n",
    "        \n",
    "        crop_amount_px = output_height_px - final_height_px\n",
    "        crop_amount_inches = crop_amount_px / dpi\n",
    "        crop_direction = \"vertical\"\n",
    "        equivalent_height_inches = output_height_px / dpi\n",
    "        equivalent_print_size = f\"{target_width_inches}\\\" x {equivalent_height_inches:.2f}\\\"\"\n",
    "    else:\n",
    "        output_height_px = final_height_px\n",
    "        scale_factor = output_height_px / input_height\n",
    "        output_width_px = int(input_width * scale_factor)\n",
    "        \n",
    "        crop_amount_px = output_width_px - final_width_px\n",
    "        crop_amount_inches = crop_amount_px / dpi\n",
    "        crop_direction = \"horizontal\"\n",
    "        equivalent_width_inches = output_width_px / dpi\n",
    "        equivalent_print_size = f\"{equivalent_width_inches:.2f}\\\" x {target_height_inches}\\\"\"\n",
    "    \n",
    "    return {\n",
    "        \"output_width_px\": output_width_px,\n",
    "        \"output_height_px\": output_height_px,\n",
    "        \"final_width_px\": final_width_px,\n",
    "        \"final_height_px\": final_height_px,\n",
    "        \"scale_factor\": scale_factor,\n",
    "        \"crop_direction\": crop_direction,\n",
    "        \"crop_amount_px\": crop_amount_px,\n",
    "        \"crop_amount_inches\": crop_amount_inches,\n",
    "        \"equivalent_print_size\": equivalent_print_size\n",
    "    }\n",
    "\n",
    "\n",
    "# ============== OUTPUT FORMAT SETTINGS ==============\n",
    "OUTPUT_FORMATS = [\"png\", \"tiff\"]\n",
    "\n",
    "# ============== UPSCALE SETTINGS ==============\n",
    "upscale_model = \"4x-UltraSharp.pth\"\n",
    "tile_size = 512\n",
    "tile_overlap = 32\n",
    "use_fp16 = True\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"2x EVALUATION MODE - Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {upscale_model}\")\n",
    "print(f\"Two-pass (16x): {USE_TWO_PASS}\")\n",
    "print(f\"DPI: {DPI}\")\n",
    "print(f\"Evaluation multiplier: {EVALUATION_MULTIPLIER}x\")\n",
    "print(f\"Output formats: {OUTPUT_FORMATS}\")\n",
    "print(f\"\\nImages to process (2x evaluation sizes):\")\n",
    "for config in IMAGE_CONFIGS:\n",
    "    print(f\"  {config['image_id'][:8]}...\")\n",
    "    print(f\"    Original target: {config['original_width']}\\\" x {config['original_height']}\\\"\")\n",
    "    print(f\"    Evaluation size: {config['eval_width_inches']}\\\" x {config['eval_height_inches']}\\\" @ {DPI}DPI\")\n",
    "    print(f\"    Output pixels: {config['target_width_px']}x{config['target_height_px']}px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Functions defined!\n"
     ]
    }
   ],
   "source": [
    "# @title Upscale Functions\n",
    "\n",
    "def upscale_with_tiles(model, img_tensor: torch.Tensor, tile_size: int, tile_overlap: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Upscale image using tiled processing to handle large images.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded spandrel model\n",
    "        img_tensor: Input image tensor (B, C, H, W)\n",
    "        tile_size: Size of each tile\n",
    "        tile_overlap: Overlap between tiles\n",
    "    \n",
    "    Returns:\n",
    "        Upscaled image tensor\n",
    "    \"\"\"\n",
    "    scale = model.scale\n",
    "    _, _, h, w = img_tensor.shape\n",
    "    \n",
    "    # If image is small enough, process directly\n",
    "    if h <= tile_size and w <= tile_size:\n",
    "        with torch.no_grad():\n",
    "            return model(img_tensor)\n",
    "    \n",
    "    # Calculate output size\n",
    "    out_h, out_w = h * scale, w * scale\n",
    "    output = torch.zeros((1, 3, out_h, out_w), device=img_tensor.device, dtype=img_tensor.dtype)\n",
    "    weight = torch.zeros((1, 1, out_h, out_w), device=img_tensor.device, dtype=img_tensor.dtype)\n",
    "    \n",
    "    # Calculate tile positions\n",
    "    stride = tile_size - tile_overlap\n",
    "    h_tiles = max(1, (h - tile_overlap) // stride + (1 if (h - tile_overlap) % stride else 0))\n",
    "    w_tiles = max(1, (w - tile_overlap) // stride + (1 if (w - tile_overlap) % stride else 0))\n",
    "    \n",
    "    total_tiles = h_tiles * w_tiles\n",
    "    print(f\"Processing {total_tiles} tiles ({h_tiles}x{w_tiles})...\")\n",
    "    \n",
    "    tile_count = 0\n",
    "    for i in range(h_tiles):\n",
    "        for j in range(w_tiles):\n",
    "            # Calculate tile boundaries\n",
    "            y1 = min(i * stride, h - tile_size) if h > tile_size else 0\n",
    "            x1 = min(j * stride, w - tile_size) if w > tile_size else 0\n",
    "            y2 = min(y1 + tile_size, h)\n",
    "            x2 = min(x1 + tile_size, w)\n",
    "            \n",
    "            # Extract and process tile\n",
    "            tile = img_tensor[:, :, y1:y2, x1:x2]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                tile_out = model(tile)\n",
    "            \n",
    "            # Calculate output positions\n",
    "            out_y1, out_y2 = y1 * scale, y2 * scale\n",
    "            out_x1, out_x2 = x1 * scale, x2 * scale\n",
    "            \n",
    "            # Create weight mask for blending\n",
    "            tile_h, tile_w = tile_out.shape[2:]\n",
    "            mask = torch.ones((1, 1, tile_h, tile_w), device=tile_out.device, dtype=tile_out.dtype)\n",
    "            \n",
    "            # Feather edges for blending\n",
    "            feather = tile_overlap * scale // 2\n",
    "            if feather > 0:\n",
    "                # Top edge\n",
    "                if i > 0:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, k, :] *= k / feather\n",
    "                # Bottom edge\n",
    "                if i < h_tiles - 1:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, -(k+1), :] *= k / feather\n",
    "                # Left edge\n",
    "                if j > 0:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, :, k] *= k / feather\n",
    "                # Right edge\n",
    "                if j < w_tiles - 1:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, :, -(k+1)] *= k / feather\n",
    "            \n",
    "            # Add to output with blending\n",
    "            output[:, :, out_y1:out_y2, out_x1:out_x2] += tile_out * mask\n",
    "            weight[:, :, out_y1:out_y2, out_x1:out_x2] += mask\n",
    "            \n",
    "            tile_count += 1\n",
    "            print(f\"  Processed {tile_count}/{total_tiles} tiles\", end='\\r')\n",
    "    \n",
    "    print(f\"  Processed {tile_count}/{total_tiles} tiles\")\n",
    "    \n",
    "    # Normalize by weights\n",
    "    output = output / weight.clamp(min=1e-8)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def save_image_formats(img_array: np.ndarray, output_name: str, output_dir: Path, formats: list) -> list:\n",
    "    \"\"\"Save image in multiple formats.\n",
    "    \n",
    "    Args:\n",
    "        img_array: Image as numpy array (H, W, C) in RGB format, uint8\n",
    "        output_name: Base filename without extension\n",
    "        output_dir: Output directory\n",
    "        formats: List of formats to save, e.g. [\"png\", \"tiff\"]\n",
    "    \n",
    "    Returns:\n",
    "        List of saved file paths\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pil_img = Image.fromarray(img_array)\n",
    "    \n",
    "    saved_paths = []\n",
    "    for fmt in formats:\n",
    "        fmt_lower = fmt.lower()\n",
    "        if fmt_lower == \"png\":\n",
    "            output_path = output_dir / f\"{output_name}.png\"\n",
    "            pil_img.save(str(output_path), \"PNG\")\n",
    "            saved_paths.append(str(output_path))\n",
    "            print(f\"   Saved: {output_path.name}\")\n",
    "        elif fmt_lower in [\"tiff\", \"tif\"]:\n",
    "            output_path = output_dir / f\"{output_name}.tiff\"\n",
    "            pil_img.save(str(output_path), \"TIFF\", compression=None)\n",
    "            saved_paths.append(str(output_path))\n",
    "            print(f\"   Saved: {output_path.name}\")\n",
    "    \n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def upscale_and_resize(\n",
    "    image_path: str,\n",
    "    target_width_inches: float,\n",
    "    target_height_inches: float,\n",
    "    dpi: int,\n",
    "    output_name: str,\n",
    "    output_formats: list,\n",
    "    model_name: str,\n",
    "    tile_size: int,\n",
    "    tile_overlap: int,\n",
    "    use_fp16: bool,\n",
    "    use_two_pass: bool\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Upscale image with Real-ESRGAN using aspect-ratio-aware scaling.\n",
    "    \n",
    "    The output will be sized so that after human cropping to the target\n",
    "    aspect ratio, the image will have exactly the target DPI.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        target_width_inches: Target print width in inches\n",
    "        target_height_inches: Target print height in inches\n",
    "        dpi: Target DPI for printing\n",
    "        output_name: Output filename (without extension)\n",
    "        output_formats: List of formats to save, e.g. [\"png\", \"tiff\"]\n",
    "        model_name: Name of upscale model\n",
    "        tile_size: Tile size for processing\n",
    "        tile_overlap: Overlap between tiles\n",
    "        use_fp16: Use FP16 precision\n",
    "        use_two_pass: If True, run 4x model twice for 16x total\n",
    "    \n",
    "    Returns:\n",
    "        dict with output paths and crop info\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Resolve image path\n",
    "    image_path = Path(image_path)\n",
    "    if not image_path.is_absolute():\n",
    "        image_path = REPO_ROOT / image_path\n",
    "    \n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {image_path}\")\n",
    "    \n",
    "    print(f\"Processing: {image_path.name}\")\n",
    "    print(f\"Target print size: {target_width_inches}\\\" x {target_height_inches}\\\" @ {dpi} DPI\")\n",
    "    \n",
    "    # Load image\n",
    "    print(\"Loading image...\")\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    input_aspect = w / h\n",
    "    target_aspect = target_width_inches / target_height_inches\n",
    "    print(f\"Input size: {w}x{h} (aspect: {input_aspect:.3f})\")\n",
    "    print(f\"Target aspect: {target_aspect:.3f}\")\n",
    "    \n",
    "    # Calculate aspect-ratio-aware output dimensions\n",
    "    scale_info = calculate_scale_for_crop(\n",
    "        input_width=w,\n",
    "        input_height=h,\n",
    "        target_width_inches=target_width_inches,\n",
    "        target_height_inches=target_height_inches,\n",
    "        dpi=dpi\n",
    "    )\n",
    "    \n",
    "    output_width = scale_info[\"output_width_px\"]\n",
    "    output_height = scale_info[\"output_height_px\"]\n",
    "    final_width = scale_info[\"final_width_px\"]\n",
    "    final_height = scale_info[\"final_height_px\"]\n",
    "    \n",
    "    print(f\"\\nðŸ“ Scaling Strategy:\")\n",
    "    print(f\"   Pre-crop output: {output_width}x{output_height}px ({scale_info['equivalent_print_size']})\")\n",
    "    print(f\"   Final after crop: {final_width}x{final_height}px ({target_width_inches}\\\" x {target_height_inches}\\\")\")\n",
    "    if scale_info[\"crop_direction\"] != \"none\":\n",
    "        print(f\"   Crop needed: {scale_info['crop_amount_px']}px {scale_info['crop_direction']} ({scale_info['crop_amount_inches']:.2f}\\\")\")\n",
    "    \n",
    "    # Calculate effective scale after upscaling\n",
    "    effective_scale = 16 if use_two_pass else 4\n",
    "    upscaled_w, upscaled_h = w * effective_scale, h * effective_scale\n",
    "    \n",
    "    if use_two_pass:\n",
    "        print(f\"\\n   Step 1: Two-pass 4x upscale (16x total) -> {upscaled_w}x{upscaled_h}\")\n",
    "    else:\n",
    "        print(f\"\\n   Step 1: 4x upscale -> {upscaled_w}x{upscaled_h}\")\n",
    "    print(f\"   Step 2: Resize to pre-crop size -> {output_width}x{output_height}\")\n",
    "    \n",
    "    # Convert to tensor\n",
    "    img_tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "    img_tensor = img_tensor.to(DEVICE)\n",
    "    \n",
    "    if use_fp16 and DEVICE.type == \"cuda\":\n",
    "        img_tensor = img_tensor.half()\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    model_path = UPSCALE_MODELS_DIR / model_name\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    model = ModelLoader().load_from_file(str(model_path))\n",
    "    assert isinstance(model, ImageModelDescriptor), \"Not an image model!\"\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    if use_fp16 and DEVICE.type == \"cuda\":\n",
    "        model = model.half()\n",
    "    model.eval()\n",
    "    \n",
    "    scale = model.scale\n",
    "    print(f\"Model scale: {scale}x\")\n",
    "    \n",
    "    # First pass upscale\n",
    "    print(\"Upscaling (pass 1 of {})...\".format(2 if use_two_pass else 1))\n",
    "    upscale_start = time.time()\n",
    "    \n",
    "    output_tensor = upscale_with_tiles(model, img_tensor, tile_size, tile_overlap)\n",
    "    \n",
    "    pass1_time = time.time() - upscale_start\n",
    "    print(f\"Pass 1 took: {pass1_time:.1f}s\")\n",
    "    \n",
    "    # Second pass if requested\n",
    "    if use_two_pass:\n",
    "        print(\"Upscaling (pass 2 of 2)...\")\n",
    "        pass2_start = time.time()\n",
    "        \n",
    "        # Use smaller tile size for second pass (image is now 4x larger)\n",
    "        tile_size_pass2 = min(tile_size, 384)  # Smaller tiles for larger image\n",
    "        output_tensor = upscale_with_tiles(model, output_tensor, tile_size_pass2, tile_overlap)\n",
    "        \n",
    "        pass2_time = time.time() - pass2_start\n",
    "        print(f\"Pass 2 took: {pass2_time:.1f}s\")\n",
    "    \n",
    "    total_upscale_time = time.time() - upscale_start\n",
    "    print(f\"Total upscaling took: {total_upscale_time:.1f}s\")\n",
    "    \n",
    "    # Convert back to image\n",
    "    output = output_tensor.squeeze(0).permute(1, 2, 0).float().cpu().numpy()\n",
    "    output = (output * 255).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    actual_upscaled_h, actual_upscaled_w = output.shape[:2]\n",
    "    print(f\"Upscaled size: {actual_upscaled_w}x{actual_upscaled_h}\")\n",
    "    \n",
    "    # Resize to pre-crop dimensions using high-quality Lanczos\n",
    "    print(f\"Resizing to pre-crop size: {output_width}x{output_height}...\")\n",
    "    pil_img = Image.fromarray(output)\n",
    "    pil_img = pil_img.resize((output_width, output_height), Image.LANCZOS)\n",
    "    \n",
    "    # Convert back to numpy (RGB)\n",
    "    output_rgb = np.array(pil_img)\n",
    "    \n",
    "    # Save output in all requested formats\n",
    "    print(f\"\\nSaving outputs ({', '.join(output_formats)})...\")\n",
    "    output_paths = save_image_formats(output_rgb, output_name, OUTPUT_DIR, output_formats)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, img_tensor, output_tensor\n",
    "    clear_memory()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… Done!\")\n",
    "    print(f\"Output size: {output_width}x{output_height}px\")\n",
    "    print(f\"Equivalent print size: {scale_info['equivalent_print_size']} @ {dpi} DPI\")\n",
    "    \n",
    "    if scale_info[\"crop_direction\"] != \"none\":\n",
    "        print(f\"\\nðŸ“‹ CROP INFO FOR HUMAN:\")\n",
    "        print(f\"   Direction: {scale_info['crop_direction'].upper()}\")\n",
    "        print(f\"   Amount to crop: {scale_info['crop_amount_px']}px ({scale_info['crop_amount_inches']:.2f}\\\")\")\n",
    "        print(f\"   After crop: {final_width}x{final_height}px = {target_width_inches}\\\" x {target_height_inches}\\\" @ {dpi} DPI\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {total_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        \"output_paths\": output_paths,\n",
    "        \"output_width\": output_width,\n",
    "        \"output_height\": output_height,\n",
    "        \"crop_direction\": scale_info[\"crop_direction\"],\n",
    "        \"crop_amount_px\": scale_info[\"crop_amount_px\"],\n",
    "        \"crop_amount_inches\": scale_info[\"crop_amount_inches\"],\n",
    "        \"final_width\": final_width,\n",
    "        \"final_height\": final_height,\n",
    "        \"equivalent_print_size\": scale_info[\"equivalent_print_size\"]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2x EVALUATION MODE - REAL-ESRGAN UPSCALING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "[1/3] 7a49a140-beee-4141-9593-3fddb9194240\n",
      "Original: 20\" x 10\"\n",
      "Evaluating at: 40\" x 20\"\n",
      "============================================================\n",
      "Processing: 7a49a140-beee-4141-9593-3fddb9194240.jpg\n",
      "Target print size: 40\" x 20\" @ 150 DPI\n",
      "Loading image...\n",
      "Input size: 682x1024 (aspect: 0.666)\n",
      "Target aspect: 2.000\n",
      "\n",
      "ðŸ“ Scaling Strategy:\n",
      "   Pre-crop output: 6000x9008px (40\" x 60.05\")\n",
      "   Final after crop: 6000x3000px (40\" x 20\")\n",
      "   Crop needed: 6008px vertical (40.05\")\n",
      "\n",
      "   Step 1: Two-pass 4x upscale (16x total) -> 10912x16384\n",
      "   Step 2: Resize to pre-crop size -> 6000x9008\n",
      "\n",
      "Loading model: 4x-UltraSharp.pth\n",
      "Model scale: 4x\n",
      "Upscaling (pass 1 of 2)...\n",
      "Processing 6 tiles (3x2)...\n",
      "  Processed 6/6 tiles\n",
      "Pass 1 took: 8.3s\n",
      "Upscaling (pass 2 of 2)...\n",
      "Processing 96 tiles (12x8)...\n",
      "  Processed 96/96 tiles\n",
      "Pass 2 took: 68.9s\n",
      "Total upscaling took: 77.2s\n",
      "Upscaled size: 10912x16384\n",
      "Resizing to pre-crop size: 6000x9008...\n",
      "\n",
      "Saving outputs (png, tiff)...\n",
      "   Saved: 7a49a140-beee-4141-9593-3fddb9194240_real-esrgan_2x-eval.png\n",
      "   Saved: 7a49a140-beee-4141-9593-3fddb9194240_real-esrgan_2x-eval.tiff\n",
      "\n",
      "âœ… Done!\n",
      "Output size: 6000x9008px\n",
      "Equivalent print size: 40\" x 60.05\" @ 150 DPI\n",
      "\n",
      "ðŸ“‹ CROP INFO FOR HUMAN:\n",
      "   Direction: VERTICAL\n",
      "   Amount to crop: 6008px (40.05\")\n",
      "   After crop: 6000x3000px = 40\" x 20\" @ 150 DPI\n",
      "\n",
      "Total time: 88.8s\n",
      "\n",
      "============================================================\n",
      "[2/3] df5cc4ff-9846-412c-ad72-e53c42184b9c\n",
      "Original: 20\" x 10\"\n",
      "Evaluating at: 40\" x 20\"\n",
      "============================================================\n",
      "Processing: df5cc4ff-9846-412c-ad72-e53c42184b9c.jpg\n",
      "Target print size: 40\" x 20\" @ 150 DPI\n",
      "Loading image...\n",
      "Input size: 1024x816 (aspect: 1.255)\n",
      "Target aspect: 2.000\n",
      "\n",
      "ðŸ“ Scaling Strategy:\n",
      "   Pre-crop output: 6000x4781px (40\" x 31.87\")\n",
      "   Final after crop: 6000x3000px (40\" x 20\")\n",
      "   Crop needed: 1781px vertical (11.87\")\n",
      "\n",
      "   Step 1: Two-pass 4x upscale (16x total) -> 16384x13056\n",
      "   Step 2: Resize to pre-crop size -> 6000x4781\n",
      "\n",
      "Loading model: 4x-UltraSharp.pth\n",
      "Model scale: 4x\n",
      "Upscaling (pass 1 of 2)...\n",
      "Processing 6 tiles (2x3)...\n",
      "  Processed 6/6 tiles\n",
      "Pass 1 took: 7.4s\n",
      "Upscaling (pass 2 of 2)...\n",
      "Processing 120 tiles (10x12)...\n",
      "  Processed 120/120 tiles\n",
      "Pass 2 took: 81.9s\n",
      "Total upscaling took: 89.3s\n",
      "Upscaled size: 16384x13056\n",
      "Resizing to pre-crop size: 6000x4781...\n",
      "\n",
      "Saving outputs (png, tiff)...\n",
      "   Saved: df5cc4ff-9846-412c-ad72-e53c42184b9c_real-esrgan_2x-eval.png\n",
      "   Saved: df5cc4ff-9846-412c-ad72-e53c42184b9c_real-esrgan_2x-eval.tiff\n",
      "\n",
      "âœ… Done!\n",
      "Output size: 6000x4781px\n",
      "Equivalent print size: 40\" x 31.87\" @ 150 DPI\n",
      "\n",
      "ðŸ“‹ CROP INFO FOR HUMAN:\n",
      "   Direction: VERTICAL\n",
      "   Amount to crop: 1781px (11.87\")\n",
      "   After crop: 6000x3000px = 40\" x 20\" @ 150 DPI\n",
      "\n",
      "Total time: 99.3s\n",
      "\n",
      "============================================================\n",
      "[3/3] b46bc519-9b4b-487f-b8d4-b95195d7e02e\n",
      "Original: 20\" x 10\"\n",
      "Evaluating at: 40\" x 20\"\n",
      "============================================================\n",
      "Processing: b46bc519-9b4b-487f-b8d4-b95195d7e02e.jpg\n",
      "Target print size: 40\" x 20\" @ 150 DPI\n",
      "Loading image...\n",
      "Input size: 1024x587 (aspect: 1.744)\n",
      "Target aspect: 2.000\n",
      "\n",
      "ðŸ“ Scaling Strategy:\n",
      "   Pre-crop output: 6000x3439px (40\" x 22.93\")\n",
      "   Final after crop: 6000x3000px (40\" x 20\")\n",
      "   Crop needed: 439px vertical (2.93\")\n",
      "\n",
      "   Step 1: Two-pass 4x upscale (16x total) -> 16384x9392\n",
      "   Step 2: Resize to pre-crop size -> 6000x3439\n",
      "\n",
      "Loading model: 4x-UltraSharp.pth\n",
      "Model scale: 4x\n",
      "Upscaling (pass 1 of 2)...\n",
      "Processing 6 tiles (2x3)...\n",
      "  Processed 6/6 tiles\n",
      "Pass 1 took: 7.4s\n",
      "Upscaling (pass 2 of 2)...\n",
      "Processing 84 tiles (7x12)...\n",
      "  Processed 84/84 tiles\n",
      "Pass 2 took: 48.9s\n",
      "Total upscaling took: 56.4s\n",
      "Upscaled size: 16384x9392\n",
      "Resizing to pre-crop size: 6000x3439...\n",
      "\n",
      "Saving outputs (png, tiff)...\n",
      "   Saved: b46bc519-9b4b-487f-b8d4-b95195d7e02e_real-esrgan_2x-eval.png\n",
      "   Saved: b46bc519-9b4b-487f-b8d4-b95195d7e02e_real-esrgan_2x-eval.tiff\n",
      "\n",
      "âœ… Done!\n",
      "Output size: 6000x3439px\n",
      "Equivalent print size: 40\" x 22.93\" @ 150 DPI\n",
      "\n",
      "ðŸ“‹ CROP INFO FOR HUMAN:\n",
      "   Direction: VERTICAL\n",
      "   Amount to crop: 439px (2.93\")\n",
      "   After crop: 6000x3000px = 40\" x 20\" @ 150 DPI\n",
      "\n",
      "Total time: 62.9s\n",
      "\n",
      "\n",
      "============================================================\n",
      "2x EVALUATION - COMPLETE\n",
      "============================================================\n",
      "\n",
      "Processed: 3/3 images successfully\n",
      "\n",
      "âœ… 7a49a140...\n",
      "   Output: 7a49a140-beee-4141-9593-3fddb9194240_real-esrgan_2x-eval.png\n",
      "   Output: 7a49a140-beee-4141-9593-3fddb9194240_real-esrgan_2x-eval.tiff\n",
      "   Size: 6000x9008 (40\" x 60.05\" @ 150 DPI)\n",
      "   Original target: 20\" x 10\"\n",
      "\n",
      "âœ… df5cc4ff...\n",
      "   Output: df5cc4ff-9846-412c-ad72-e53c42184b9c_real-esrgan_2x-eval.png\n",
      "   Output: df5cc4ff-9846-412c-ad72-e53c42184b9c_real-esrgan_2x-eval.tiff\n",
      "   Size: 6000x4781 (40\" x 31.87\" @ 150 DPI)\n",
      "   Original target: 20\" x 10\"\n",
      "\n",
      "âœ… b46bc519...\n",
      "   Output: b46bc519-9b4b-487f-b8d4-b95195d7e02e_real-esrgan_2x-eval.png\n",
      "   Output: b46bc519-9b4b-487f-b8d4-b95195d7e02e_real-esrgan_2x-eval.tiff\n",
      "   Size: 6000x3439 (40\" x 22.93\" @ 150 DPI)\n",
      "   Original target: 20\" x 10\"\n",
      "\n",
      "\n",
      "ðŸ“‹ Next steps:\n",
      "   1. Open the output in Photoshop\n",
      "   2. View at 100% to check for pixelation\n",
      "   3. If quality is acceptable, run the standard notebook for final output\n"
     ]
    }
   ],
   "source": [
    "# @title Run 2x Evaluation Upscaling\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"2x EVALUATION MODE - REAL-ESRGAN UPSCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(IMAGE_CONFIGS, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{i}/{len(IMAGE_CONFIGS)}] {config['image_id']}\")\n",
    "    print(f\"Original: {config['original_width']}\\\" x {config['original_height']}\\\"\")\n",
    "    print(f\"Evaluating at: {config['eval_width_inches']}\\\" x {config['eval_height_inches']}\\\"\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Use _2x-eval suffix for evaluation outputs\n",
    "        output_name = f\"{config['image_id']}_real-esrgan_2x-eval\"\n",
    "        \n",
    "        result = upscale_and_resize(\n",
    "            image_path=config[\"input_path\"],\n",
    "            target_width_inches=config[\"eval_width_inches\"],\n",
    "            target_height_inches=config[\"eval_height_inches\"],\n",
    "            dpi=DPI,\n",
    "            output_name=output_name,\n",
    "            output_formats=OUTPUT_FORMATS,\n",
    "            model_name=upscale_model,\n",
    "            tile_size=tile_size,\n",
    "            tile_overlap=tile_overlap,\n",
    "            use_fp16=use_fp16,\n",
    "            use_two_pass=USE_TWO_PASS\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"image_id\": config[\"image_id\"],\n",
    "            \"status\": \"Success\",\n",
    "            \"outputs\": result[\"output_paths\"],\n",
    "            \"output_size\": f\"{result['output_width']}x{result['output_height']}\",\n",
    "            \"eval_size\": result[\"equivalent_print_size\"],\n",
    "            \"original_target\": f\"{config['original_width']}\\\" x {config['original_height']}\\\"\"\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results.append({\n",
    "            \"image_id\": config[\"image_id\"],\n",
    "            \"status\": \"Failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"2x EVALUATION - COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "success_count = sum(1 for r in results if r[\"status\"] == \"Success\")\n",
    "print(f\"\\nProcessed: {success_count}/{len(results)} images successfully\\n\")\n",
    "\n",
    "for r in results:\n",
    "    if r[\"status\"] == \"Success\":\n",
    "        print(f\"âœ… {r['image_id'][:8]}...\")\n",
    "        for out_path in r[\"outputs\"]:\n",
    "            print(f\"   Output: {Path(out_path).name}\")\n",
    "        print(f\"   Size: {r['output_size']} ({r['eval_size']} @ {DPI} DPI)\")\n",
    "        print(f\"   Original target: {r['original_target']}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"âŒ {r['image_id'][:8]}... -> {r.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Next steps:\")\n",
    "print(\"   1. Open the output in Photoshop\")\n",
    "print(\"   2. View at 100% to check for pixelation\")\n",
    "print(\"   3. If quality is acceptable, run the standard notebook for final output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Processing (Optional)\n",
    "\n",
    "Use the cell below if you want to process a single image with custom settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Single Image Upscale (Optional)\n",
    "\n",
    "# Uncomment and modify to process a single image:\n",
    "\n",
    "# single_result = upscale_and_resize(\n",
    "#     image_path=\"input/your_image.jpg\",\n",
    "#     target_width=1650,   # 11 inches * 150 DPI\n",
    "#     target_height=2100,  # 14 inches * 150 DPI\n",
    "#     output_name=\"your_image_real-esrgan\",\n",
    "#     model_name=\"4x-UltraSharp.pth\",\n",
    "#     tile_size=512,\n",
    "#     tile_overlap=32,\n",
    "#     use_fp16=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
