{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Real-ESRGAN Standalone Upscaler (Fast Mode)**\n",
    "\n",
    "A fast upscaling solution using Real-ESRGAN/ESRGAN models directly without diffusion.\n",
    "\n",
    "## Speed Comparison\n",
    "- **This notebook**: ~10-60 seconds per image (4x upscale)\n",
    "- **FLUX notebook**: ~10-20 minutes per image (4x upscale)\n",
    "\n",
    "## When to Use\n",
    "- Fast batch processing\n",
    "- When original image quality is already good\n",
    "- Preview/proof before final FLUX upscale\n",
    "- When you don't need AI-generated detail enhancement\n",
    "\n",
    "## Available Models\n",
    "- `4x-UltraSharp.pth` - Sharp details, good for digital art (default)\n",
    "- `4x_foolhardy_Remacri.pth` - Natural textures, less over-sharpening\n",
    "- `4x-AnimeSharp.pth` - Optimized for anime/illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\Armaan\\Desktop\\Artinafti\n",
      "Models directory: C:\\Users\\Armaan\\Desktop\\Artinafti\\hf\\models\\upscale_models\n",
      "Output directory: C:\\Users\\Armaan\\Desktop\\Artinafti\\4xoutput\n",
      "Installing packages...\n",
      "Installing PyTorch nightly with CUDA 12.8...\n",
      "âœ“ PyTorch installed\n",
      "âœ“ spandrel installed\n",
      "âœ“ opencv-python installed\n",
      "âœ“ huggingface_hub installed\n",
      "âœ“ safetensors installed\n",
      "âœ“ pillow installed\n",
      "\n",
      "âœ… Environment Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Environment (Run Once)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get repo root\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "# Directory paths\n",
    "HF_DIR = REPO_ROOT / \"hf\"\n",
    "MODELS_DIR = HF_DIR / \"models\"\n",
    "UPSCALE_MODELS_DIR = MODELS_DIR / \"upscale_models\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"4xoutput\"\n",
    "INPUT_DIR = REPO_ROOT / \"input\"\n",
    "\n",
    "# Create directories\n",
    "for d in [HF_DIR, MODELS_DIR, UPSCALE_MODELS_DIR, OUTPUT_DIR, INPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Models directory: {UPSCALE_MODELS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for Real-ESRGAN standalone.\"\"\"\n",
    "    # PyTorch with CUDA 12.8 for RTX 5060 Ti\n",
    "    print(\"Installing PyTorch nightly with CUDA 12.8...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q', '--pre',\n",
    "         'torch', 'torchvision',\n",
    "         '--index-url', 'https://download.pytorch.org/whl/nightly/cu128'],\n",
    "        capture_output=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ“ PyTorch installed\")\n",
    "    else:\n",
    "        print(f\"âœ— PyTorch install failed: {result.stderr.decode()}\")\n",
    "    \n",
    "    # Spandrel for loading upscale models\n",
    "    packages = ['spandrel', 'opencv-python', 'huggingface_hub', 'safetensors', 'pillow']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                check=True, capture_output=True\n",
    "            )\n",
    "            print(f\"âœ“ {package} installed\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âœ— Error installing {package}\")\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "install_packages()\n",
    "print(\"\\nâœ… Environment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading upscale models...\n",
      "âœ“ 4x-UltraSharp.pth already exists\n",
      "âœ“ 4x_foolhardy_Remacri.pth already exists\n",
      "âœ“ 4x-AnimeSharp.pth already exists\n",
      "\n",
      "âœ… Models downloaded!\n"
     ]
    }
   ],
   "source": [
    "# @title Download Models (Run Once)\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Ensure paths are set\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "UPSCALE_MODELS_DIR = REPO_ROOT / \"hf\" / \"models\" / \"upscale_models\"\n",
    "UPSCALE_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_model(repo_id: str, filename: str, dest_dir: Path) -> str:\n",
    "    \"\"\"Download model from HuggingFace Hub.\"\"\"\n",
    "    dest_path = dest_dir / filename\n",
    "    if dest_path.exists():\n",
    "        print(f\"âœ“ {filename} already exists\")\n",
    "        return filename\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
    "        hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=filename,\n",
    "            local_dir=str(dest_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        print(\"Done!\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Downloading upscale models...\")\n",
    "download_model(\"Isi99999/Upscalers\", \"4x-UltraSharp.pth\", UPSCALE_MODELS_DIR)\n",
    "download_model(\"Isi99999/Upscalers\", \"4x_foolhardy_Remacri.pth\", UPSCALE_MODELS_DIR)\n",
    "download_model(\"Isi99999/Upscalers\", \"4x-AnimeSharp.pth\", UPSCALE_MODELS_DIR)\n",
    "\n",
    "print(\"\\nâœ… Models downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded!\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "VRAM: 15.9 GB\n"
     ]
    }
   ],
   "source": [
    "# @title Load Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Import spandrel for model loading\n",
    "from spandrel import ImageModelDescriptor, ModelLoader\n",
    "\n",
    "# Set up paths\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "if NOTEBOOK_DIR.name == \"resolution-upscaling\":\n",
    "    REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    REPO_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "UPSCALE_MODELS_DIR = REPO_ROOT / \"hf\" / \"models\" / \"upscale_models\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"4xoutput\"\n",
    "INPUT_DIR = REPO_ROOT / \"input\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"âœ… Libraries loaded!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set!\n",
      "Input: C:\\Users\\Armaan\\Desktop\\Artinafti\\9ab756e9-8505-48bd-aa38-3c360d1e29b6.jpg\n",
      "Output dir: C:\\Users\\Armaan\\Desktop\\Artinafti\\6x\n",
      "Mode: Pure 6x upscale (two-pass 16x -> Lanczos to 6x, no downsizing)\n"
     ]
    }
   ],
   "source": [
    "# @title Configuration - 6x Upscale (Single Image, No Downsizing)\n",
    "\n",
    "INPUT_IMAGE = str(REPO_ROOT / \"9ab756e9-8505-48bd-aa38-3c360d1e29b6.jpg\")\n",
    "OUTPUT_DIR = REPO_ROOT / \"6x\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_FORMATS = [\"png\", \"tiff\"]\n",
    "\n",
    "# ============== UPSCALE SETTINGS ==============\n",
    "# Model options: \"4x-UltraSharp.pth\", \"4x_foolhardy_Remacri.pth\", \"4x-AnimeSharp.pth\"\n",
    "upscale_model = \"4x-UltraSharp.pth\"\n",
    "\n",
    "# Tile size for processing\n",
    "tile_size = 512\n",
    "tile_overlap = 32\n",
    "\n",
    "# Use FP16 for faster processing\n",
    "use_fp16 = True\n",
    "\n",
    "# Two-pass: 4x model twice (16x total) then Lanczos down to 6x for best quality\n",
    "USE_TWO_PASS = True\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"Input: {INPUT_IMAGE}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Mode: Pure 6x upscale (two-pass 16x -> Lanczos to 6x, no downsizing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Functions defined!\n"
     ]
    }
   ],
   "source": [
    "# @title Upscale Functions\n",
    "\n",
    "def upscale_with_tiles(model, img_tensor: torch.Tensor, tile_size: int, tile_overlap: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Upscale image using tiled processing to handle large images.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded spandrel model\n",
    "        img_tensor: Input image tensor (B, C, H, W)\n",
    "        tile_size: Size of each tile\n",
    "        tile_overlap: Overlap between tiles\n",
    "    \n",
    "    Returns:\n",
    "        Upscaled image tensor\n",
    "    \"\"\"\n",
    "    scale = model.scale\n",
    "    _, _, h, w = img_tensor.shape\n",
    "    \n",
    "    # If image is small enough, process directly\n",
    "    if h <= tile_size and w <= tile_size:\n",
    "        with torch.no_grad():\n",
    "            return model(img_tensor)\n",
    "    \n",
    "    # Calculate output size\n",
    "    out_h, out_w = h * scale, w * scale\n",
    "    output = torch.zeros((1, 3, out_h, out_w), device=img_tensor.device, dtype=img_tensor.dtype)\n",
    "    weight = torch.zeros((1, 1, out_h, out_w), device=img_tensor.device, dtype=img_tensor.dtype)\n",
    "    \n",
    "    # Calculate tile positions\n",
    "    stride = tile_size - tile_overlap\n",
    "    h_tiles = max(1, (h - tile_overlap) // stride + (1 if (h - tile_overlap) % stride else 0))\n",
    "    w_tiles = max(1, (w - tile_overlap) // stride + (1 if (w - tile_overlap) % stride else 0))\n",
    "    \n",
    "    total_tiles = h_tiles * w_tiles\n",
    "    print(f\"Processing {total_tiles} tiles ({h_tiles}x{w_tiles})...\")\n",
    "    \n",
    "    tile_count = 0\n",
    "    for i in range(h_tiles):\n",
    "        for j in range(w_tiles):\n",
    "            # Calculate tile boundaries\n",
    "            y1 = min(i * stride, h - tile_size) if h > tile_size else 0\n",
    "            x1 = min(j * stride, w - tile_size) if w > tile_size else 0\n",
    "            y2 = min(y1 + tile_size, h)\n",
    "            x2 = min(x1 + tile_size, w)\n",
    "            \n",
    "            # Extract and process tile\n",
    "            tile = img_tensor[:, :, y1:y2, x1:x2]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                tile_out = model(tile)\n",
    "            \n",
    "            # Calculate output positions\n",
    "            out_y1, out_y2 = y1 * scale, y2 * scale\n",
    "            out_x1, out_x2 = x1 * scale, x2 * scale\n",
    "            \n",
    "            # Create weight mask for blending\n",
    "            tile_h, tile_w = tile_out.shape[2:]\n",
    "            mask = torch.ones((1, 1, tile_h, tile_w), device=tile_out.device, dtype=tile_out.dtype)\n",
    "            \n",
    "            # Feather edges for blending\n",
    "            feather = tile_overlap * scale // 2\n",
    "            if feather > 0:\n",
    "                # Top edge\n",
    "                if i > 0:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, k, :] *= k / feather\n",
    "                # Bottom edge\n",
    "                if i < h_tiles - 1:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, -(k+1), :] *= k / feather\n",
    "                # Left edge\n",
    "                if j > 0:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, :, k] *= k / feather\n",
    "                # Right edge\n",
    "                if j < w_tiles - 1:\n",
    "                    for k in range(feather):\n",
    "                        mask[:, :, :, -(k+1)] *= k / feather\n",
    "            \n",
    "            # Add to output with blending\n",
    "            output[:, :, out_y1:out_y2, out_x1:out_x2] += tile_out * mask\n",
    "            weight[:, :, out_y1:out_y2, out_x1:out_x2] += mask\n",
    "            \n",
    "            tile_count += 1\n",
    "            print(f\"  Processed {tile_count}/{total_tiles} tiles\", end='\\r')\n",
    "    \n",
    "    print(f\"  Processed {tile_count}/{total_tiles} tiles\")\n",
    "    \n",
    "    # Normalize by weights\n",
    "    output = output / weight.clamp(min=1e-8)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def save_image_formats(img_array: np.ndarray, output_name: str, output_dir: Path, formats: list) -> list:\n",
    "    \"\"\"Save image in multiple formats.\n",
    "    \n",
    "    Args:\n",
    "        img_array: Image as numpy array (H, W, C) in RGB format, uint8\n",
    "        output_name: Base filename without extension\n",
    "        output_dir: Output directory\n",
    "        formats: List of formats to save, e.g. [\"png\", \"tiff\"]\n",
    "    \n",
    "    Returns:\n",
    "        List of saved file paths\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pil_img = Image.fromarray(img_array)\n",
    "    \n",
    "    saved_paths = []\n",
    "    for fmt in formats:\n",
    "        fmt_lower = fmt.lower()\n",
    "        if fmt_lower == \"png\":\n",
    "            output_path = output_dir / f\"{output_name}.png\"\n",
    "            pil_img.save(str(output_path), \"PNG\")\n",
    "            saved_paths.append(str(output_path))\n",
    "            print(f\"   Saved: {output_path.name}\")\n",
    "        elif fmt_lower in [\"tiff\", \"tif\"]:\n",
    "            output_path = output_dir / f\"{output_name}.tiff\"\n",
    "            pil_img.save(str(output_path), \"TIFF\", compression=None)\n",
    "            saved_paths.append(str(output_path))\n",
    "            print(f\"   Saved: {output_path.name}\")\n",
    "    \n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def upscale_and_resize(\n",
    "    image_path: str,\n",
    "    target_width_inches: float,\n",
    "    target_height_inches: float,\n",
    "    dpi: int,\n",
    "    output_name: str,\n",
    "    output_formats: list,\n",
    "    model_name: str,\n",
    "    tile_size: int,\n",
    "    tile_overlap: int,\n",
    "    use_fp16: bool,\n",
    "    use_two_pass: bool\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Upscale image with Real-ESRGAN using aspect-ratio-aware scaling.\n",
    "    \n",
    "    The output will be sized so that after human cropping to the target\n",
    "    aspect ratio, the image will have exactly the target DPI.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        target_width_inches: Target print width in inches\n",
    "        target_height_inches: Target print height in inches\n",
    "        dpi: Target DPI for printing\n",
    "        output_name: Output filename (without extension)\n",
    "        output_formats: List of formats to save, e.g. [\"png\", \"tiff\"]\n",
    "        model_name: Name of upscale model\n",
    "        tile_size: Tile size for processing\n",
    "        tile_overlap: Overlap between tiles\n",
    "        use_fp16: Use FP16 precision\n",
    "        use_two_pass: If True, run 4x model twice for 16x total\n",
    "    \n",
    "    Returns:\n",
    "        dict with output paths and crop info\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Resolve image path\n",
    "    image_path = Path(image_path)\n",
    "    if not image_path.is_absolute():\n",
    "        image_path = REPO_ROOT / image_path\n",
    "    \n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {image_path}\")\n",
    "    \n",
    "    print(f\"Processing: {image_path.name}\")\n",
    "    print(f\"Target print size: {target_width_inches}\\\" x {target_height_inches}\\\" @ {dpi} DPI\")\n",
    "    \n",
    "    # Load image\n",
    "    print(\"Loading image...\")\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    input_aspect = w / h\n",
    "    target_aspect = target_width_inches / target_height_inches\n",
    "    print(f\"Input size: {w}x{h} (aspect: {input_aspect:.3f})\")\n",
    "    print(f\"Target aspect: {target_aspect:.3f}\")\n",
    "    \n",
    "    # Calculate aspect-ratio-aware output dimensions\n",
    "    scale_info = calculate_scale_for_crop(\n",
    "        input_width=w,\n",
    "        input_height=h,\n",
    "        target_width_inches=target_width_inches,\n",
    "        target_height_inches=target_height_inches,\n",
    "        dpi=dpi\n",
    "    )\n",
    "    \n",
    "    output_width = scale_info[\"output_width_px\"]\n",
    "    output_height = scale_info[\"output_height_px\"]\n",
    "    final_width = scale_info[\"final_width_px\"]\n",
    "    final_height = scale_info[\"final_height_px\"]\n",
    "    \n",
    "    print(f\"\\nðŸ“ Scaling Strategy:\")\n",
    "    print(f\"   Pre-crop output: {output_width}x{output_height}px ({scale_info['equivalent_print_size']})\")\n",
    "    print(f\"   Final after crop: {final_width}x{final_height}px ({target_width_inches}\\\" x {target_height_inches}\\\")\")\n",
    "    if scale_info[\"crop_direction\"] != \"none\":\n",
    "        print(f\"   Crop needed: {scale_info['crop_amount_px']}px {scale_info['crop_direction']} ({scale_info['crop_amount_inches']:.2f}\\\")\")\n",
    "    \n",
    "    # Calculate effective scale after upscaling\n",
    "    effective_scale = 16 if use_two_pass else 4\n",
    "    upscaled_w, upscaled_h = w * effective_scale, h * effective_scale\n",
    "    \n",
    "    if use_two_pass:\n",
    "        print(f\"\\n   Step 1: Two-pass 4x upscale (16x total) -> {upscaled_w}x{upscaled_h}\")\n",
    "    else:\n",
    "        print(f\"\\n   Step 1: 4x upscale -> {upscaled_w}x{upscaled_h}\")\n",
    "    print(f\"   Step 2: Resize to pre-crop size -> {output_width}x{output_height}\")\n",
    "    \n",
    "    # Convert to tensor\n",
    "    img_tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "    img_tensor = img_tensor.to(DEVICE)\n",
    "    \n",
    "    if use_fp16 and DEVICE.type == \"cuda\":\n",
    "        img_tensor = img_tensor.half()\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    model_path = UPSCALE_MODELS_DIR / model_name\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    model = ModelLoader().load_from_file(str(model_path))\n",
    "    assert isinstance(model, ImageModelDescriptor), \"Not an image model!\"\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    if use_fp16 and DEVICE.type == \"cuda\":\n",
    "        model = model.half()\n",
    "    model.eval()\n",
    "    \n",
    "    scale = model.scale\n",
    "    print(f\"Model scale: {scale}x\")\n",
    "    \n",
    "    # First pass upscale\n",
    "    print(\"Upscaling (pass 1 of {})...\".format(2 if use_two_pass else 1))\n",
    "    upscale_start = time.time()\n",
    "    \n",
    "    output_tensor = upscale_with_tiles(model, img_tensor, tile_size, tile_overlap)\n",
    "    \n",
    "    pass1_time = time.time() - upscale_start\n",
    "    print(f\"Pass 1 took: {pass1_time:.1f}s\")\n",
    "    \n",
    "    # Second pass if requested\n",
    "    if use_two_pass:\n",
    "        print(\"Upscaling (pass 2 of 2)...\")\n",
    "        pass2_start = time.time()\n",
    "        \n",
    "        # Use smaller tile size for second pass (image is now 4x larger)\n",
    "        tile_size_pass2 = min(tile_size, 384)  # Smaller tiles for larger image\n",
    "        output_tensor = upscale_with_tiles(model, output_tensor, tile_size_pass2, tile_overlap)\n",
    "        \n",
    "        pass2_time = time.time() - pass2_start\n",
    "        print(f\"Pass 2 took: {pass2_time:.1f}s\")\n",
    "    \n",
    "    total_upscale_time = time.time() - upscale_start\n",
    "    print(f\"Total upscaling took: {total_upscale_time:.1f}s\")\n",
    "    \n",
    "    # Convert back to image\n",
    "    output = output_tensor.squeeze(0).permute(1, 2, 0).float().cpu().numpy()\n",
    "    output = (output * 255).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    actual_upscaled_h, actual_upscaled_w = output.shape[:2]\n",
    "    print(f\"Upscaled size: {actual_upscaled_w}x{actual_upscaled_h}\")\n",
    "    \n",
    "    # Resize to pre-crop dimensions using high-quality Lanczos\n",
    "    print(f\"Resizing to pre-crop size: {output_width}x{output_height}...\")\n",
    "    pil_img = Image.fromarray(output)\n",
    "    pil_img = pil_img.resize((output_width, output_height), Image.LANCZOS)\n",
    "    \n",
    "    # Convert back to numpy (RGB)\n",
    "    output_rgb = np.array(pil_img)\n",
    "    \n",
    "    # Save output in all requested formats\n",
    "    print(f\"\\nSaving outputs ({', '.join(output_formats)})...\")\n",
    "    output_paths = save_image_formats(output_rgb, output_name, OUTPUT_DIR, output_formats)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, img_tensor, output_tensor\n",
    "    clear_memory()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… Done!\")\n",
    "    print(f\"Output size: {output_width}x{output_height}px\")\n",
    "    print(f\"Equivalent print size: {scale_info['equivalent_print_size']} @ {dpi} DPI\")\n",
    "    \n",
    "    if scale_info[\"crop_direction\"] != \"none\":\n",
    "        print(f\"\\nðŸ“‹ CROP INFO FOR HUMAN:\")\n",
    "        print(f\"   Direction: {scale_info['crop_direction'].upper()}\")\n",
    "        print(f\"   Amount to crop: {scale_info['crop_amount_px']}px ({scale_info['crop_amount_inches']:.2f}\\\")\")\n",
    "        print(f\"   After crop: {final_width}x{final_height}px = {target_width_inches}\\\" x {target_height_inches}\\\" @ {dpi} DPI\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {total_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        \"output_paths\": output_paths,\n",
    "        \"output_width\": output_width,\n",
    "        \"output_height\": output_height,\n",
    "        \"crop_direction\": scale_info[\"crop_direction\"],\n",
    "        \"crop_amount_px\": scale_info[\"crop_amount_px\"],\n",
    "        \"crop_amount_inches\": scale_info[\"crop_amount_inches\"],\n",
    "        \"final_width\": final_width,\n",
    "        \"final_height\": final_height,\n",
    "        \"equivalent_print_size\": scale_info[\"equivalent_print_size\"]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6X UPSCALING WITH REAL-ESRGAN\n",
      "============================================================\n",
      "\n",
      "Processing: 9ab756e9-8505-48bd-aa38-3c360d1e29b6.jpg\n",
      "Input size: 678x1024\n",
      "Target 6x size: 4068x6144\n",
      "\n",
      "Loading model: 4x-UltraSharp.pth\n",
      "Model scale: 4x\n",
      "\n",
      "Upscaling (pass 1 of 2)...\n",
      "Processing 6 tiles (3x2)...\n",
      "  Processed 6/6 tiles\n",
      "Pass 1 output: 2712x4096 (took 12.2s)\n",
      "Upscaling (pass 2 of 2)...\n",
      "Processing 96 tiles (12x8)...\n",
      "  Processed 56/96 tiles"
     ]
    }
   ],
   "source": [
    "# @title Run 6x Upscaling with Real-ESRGAN\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"6X UPSCALING WITH REAL-ESRGAN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "image_path = Path(INPUT_IMAGE)\n",
    "if not image_path.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {image_path}\")\n",
    "\n",
    "# Load image\n",
    "print(f\"\\nProcessing: {image_path.name}\")\n",
    "img = cv2.imread(str(image_path))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "h, w = img.shape[:2]\n",
    "target_w, target_h = w * 6, h * 6\n",
    "\n",
    "print(f\"Input size: {w}x{h}\")\n",
    "print(f\"Target 6x size: {target_w}x{target_h}\")\n",
    "\n",
    "# Convert to tensor\n",
    "img_tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "img_tensor = img_tensor.to(DEVICE)\n",
    "if use_fp16 and DEVICE.type == \"cuda\":\n",
    "    img_tensor = img_tensor.half()\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model: {upscale_model}\")\n",
    "model_path = UPSCALE_MODELS_DIR / upscale_model\n",
    "model = ModelLoader().load_from_file(str(model_path))\n",
    "assert isinstance(model, ImageModelDescriptor), \"Not an image model!\"\n",
    "model = model.to(DEVICE)\n",
    "if use_fp16 and DEVICE.type == \"cuda\":\n",
    "    model = model.half()\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model scale: {model.scale}x\")\n",
    "\n",
    "# Pass 1: 4x upscale\n",
    "print(\"\\nUpscaling (pass 1 of {})...\".format(2 if USE_TWO_PASS else 1))\n",
    "start_time = time.time()\n",
    "output_tensor = upscale_with_tiles(model, img_tensor, tile_size, tile_overlap)\n",
    "pass1_time = time.time() - start_time\n",
    "p1_h, p1_w = output_tensor.shape[2], output_tensor.shape[3]\n",
    "print(f\"Pass 1 output: {p1_w}x{p1_h} (took {pass1_time:.1f}s)\")\n",
    "\n",
    "# Pass 2 if two-pass mode (16x total, then downscale to 6x)\n",
    "if USE_TWO_PASS:\n",
    "    print(\"Upscaling (pass 2 of 2)...\")\n",
    "    pass2_start = time.time()\n",
    "    tile_size_pass2 = min(tile_size, 384)\n",
    "    output_tensor = upscale_with_tiles(model, output_tensor, tile_size_pass2, tile_overlap)\n",
    "    pass2_time = time.time() - pass2_start\n",
    "    p2_h, p2_w = output_tensor.shape[2], output_tensor.shape[3]\n",
    "    print(f\"Pass 2 output: {p2_w}x{p2_h} (took {pass2_time:.1f}s)\")\n",
    "\n",
    "total_upscale_time = time.time() - start_time\n",
    "print(f\"Total upscaling took: {total_upscale_time:.1f}s\")\n",
    "\n",
    "# Convert to numpy - use chunked approach to avoid memory error\n",
    "print(\"Converting to image...\")\n",
    "output_tensor = output_tensor.float()  # Ensure float32 for conversion\n",
    "output_np = output_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "del output_tensor\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "output_np = np.clip(output_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "actual_h, actual_w = output_np.shape[:2]\n",
    "print(f\"Upscaled size: {actual_w}x{actual_h}\")\n",
    "\n",
    "# Lanczos resize to exact 6x\n",
    "print(f\"Resizing to 6x ({target_w}x{target_h}) with Lanczos...\")\n",
    "pil_img = Image.fromarray(output_np)\n",
    "del output_np\n",
    "gc.collect()\n",
    "\n",
    "pil_img = pil_img.resize((target_w, target_h), Image.LANCZOS)\n",
    "output_rgb = np.array(pil_img)\n",
    "\n",
    "# Save\n",
    "output_name = \"9ab756e9-8505-48bd-aa38-3c360d1e29b6_real-esrgan-6x\"\n",
    "print(f\"\\nSaving outputs ({', '.join(OUTPUT_FORMATS)})...\")\n",
    "save_image_formats(output_rgb, output_name, OUTPUT_DIR, OUTPUT_FORMATS)\n",
    "\n",
    "# Cleanup\n",
    "del model, img_tensor, pil_img, output_rgb\n",
    "clear_memory()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Done! Total time: {total_time:.1f}s\")\n",
    "print(f\"Output: {target_w}x{target_h} (6x upscale, no downsizing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Processing (Optional)\n",
    "\n",
    "Use the cell below if you want to process a single image with custom settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Single Image Upscale (Optional)\n",
    "\n",
    "# Uncomment and modify to process a single image:\n",
    "\n",
    "# single_result = upscale_and_resize(\n",
    "#     image_path=\"input/your_image.jpg\",\n",
    "#     target_width=1650,   # 11 inches * 150 DPI\n",
    "#     target_height=2100,  # 14 inches * 150 DPI\n",
    "#     output_name=\"your_image_real-esrgan\",\n",
    "#     model_name=\"4x-UltraSharp.pth\",\n",
    "#     tile_size=512,\n",
    "#     tile_overlap=32,\n",
    "#     use_fp16=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
